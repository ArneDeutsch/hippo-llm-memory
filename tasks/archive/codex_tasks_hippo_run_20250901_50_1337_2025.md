
# Codex Tasks — Mitigations for 20250901_50_1337_2025 Run
**Repo:** `hippo-llm-memory`  
**Context:** Address gaps identified in *Run Audit & Pipeline Review — 20250901_50_1337_2025*.  
**Deliverable:** One PR per EPIC (or grouped as noted), each with passing unit tests and updated docs.

> Source of findings: `review/hippo_run_audit_20250901.md` (added by the user).

---

## EPIC A — Compute real **pre‑phase** metrics and rollups
**Rationale:** Baselines show EM/F1=0 because per‑item metrics are not computed in pre‑phase, and rollups coerce missing `pre_*` to 0.

### A1. Enable pre‑phase per‑item scoring in harness
- **Files (expected):**
  - `hippo_mem/eval/harness.py`
  - `hippo_mem/eval/runner.py` (if separate)
  - `configs/eval/*.yaml` (default flags)
- **Changes:**
  1. Ensure the *pre* sweep calls the evaluator with `compute_metrics=True` so `metrics.csv` contains `em_raw`, `em_norm`, `f1` per row.
  2. Persist `pre_*` aggregates into `metrics.json` next to `post_*` (or under a `phase: {pre,post}` key).
  3. Add CLI flag `--pre-metrics/--no-pre-metrics` (default: **on**).
- **Definition of Done (DoD):**
  - Running (size=5) baseline yields **non‑NaN** per‑item metrics in `runs/*/baselines/*/*/metrics.csv`.
  - `metrics.json` contains nonzero `pre_em`, `pre_f1` where appropriate.

### A2. Fix report aggregation for missing `pre_*`
- **Files:**
  - `scripts/reporting/aggregate.py`
  - `reports/templates/*.jinja` or similar renderers
- **Changes:**
  1. Treat missing pre values as **missing**, not zero; render “_missing_” + add ⚠️ badge.
  2. Add guard: if any suite has `pre_*` missing across >20% items, exit with non‑zero and message.
- **DoD:**
  - Aggregates render with 3 decimals; missing pre metrics are flagged, not zeroed.
  - CI job `report-smoke` passes with updated formatting.

---

## EPIC B — **Replay & persistence** correctness
**Rationale:** Stores contain stubs; `replay.samples=0` yet data persisted.

### B1. Gate persistence on replay activity
- **Files:**
  - `hippo_mem/episodic/store.py`
  - `hippo_mem/relational/store.py`
  - `hippo_mem/spatial/store.py`
  - `hippo_mem/eval/harness.py` (writes)
- **Changes:**
  1. Persist only if `replay.samples > 0` (or equivalent signal).
  2. Write a sidecar `store_meta.json` with: `{ "schema": "...", "replay_samples": N, "source": "replay|stub", "created_at": ISO }`.
- **DoD:**
  - With `replay.samples=0`, no `*.jsonl` data files are created; only `store_meta.json` exists with `"source": "stub"`.
  - With `replay.samples>=1`, data files exist and `store_meta.json.source == "replay"`.

### B2. Remove dummy writes from module init for production
- **Files:** `hippo_mem/*/_init_modules.py` or equivalent
- **Changes:**
  1. Move dummy record insertion behind `--allow-dummy-stores` (default **off**).
  2. Reporter filters: ignore records with `value.provenance == "dummy"` if any slipped through.
- **DoD:**
  - Default runs contain **no** `provenance: "dummy"` entries.

---

## EPIC C — **Spatial** task target/metric alignment
**Rationale:** Predictions are formatted as full paths; gold expects indices/coords/moves ⇒ EM/F1=0.

### C1. Choose and enforce a canonical spatial target
- **Decision:** Use **move string** target (`U/D/L/R`), with optional compression like `U4D1L2` for display. (Alternative acceptable: final `[x,y]` coord; pick one and apply consistently.)
- **Files:**
  - `hippo_mem/tasks/spatial/generator.py`
  - `hippo_mem/tasks/spatial/prompt.py`
  - `hippo_mem/metrics/spatial.py`
- **Changes:**
  1. Generator: emit gold as canonical target.
  2. Prompt: instruct the model to output **exactly** the canonical format.
  3. Scorer: normalize predictions (strip punctuation/whitespace, map synonyms) and compute EM/F1 over tokens.
  4. Add `ensure_prediction_format()` that converts verbose path outputs to canonical moves if possible.
- **DoD:**
  - Unit tests: 10 examples where verbose path → canonical target yields EM=1.
  - On a smoke run (n=20), baseline EM > 0.05 (non‑zero), memory variant change is measurable.

---

## EPIC D — Raise **semantic** bench hardness
**Rationale:** Ceiling effects (EM≈1.0 for memory variant), obscuring uplift.

### D1. Implement `dataset_profile: hard`
- **Files:** `configs/datasets/semantic.yaml`, `hippo_mem/tasks/semantic/generator.py`
- **Changes:**
  1. Add distractors/decoys and entity overlap controls.
  2. Parameterize paraphrase noise and mention ambiguity.
  3. Wire via Hydra/CLI: `+dataset_profile=hard`.
- **DoD:**
  - On `SIZES=50`, baseline EM in [0.3, 0.7] (using the default test model).

---

## EPIC E — **Gating** telemetry & invariants
**Rationale:** `GateNoOp`/empty counters suggest gates aren’t exercised.

### E1. Emit gate counters
- **Files:** `hippo_mem/*/gating.py` or retrieval path
- **Counters:** `attempts`, `accepted`, `blocked`, `skipped`, `null_input`.
- **DoD:** `metrics.json.gating` shows consistent non‑zero counters when memory is enabled.

### E2. Invariant checks
- **Files:** `hippo_mem/eval/harness.py`
- **Rules:**
  - If memory enabled and `retrieval_tokens == 0` for a suite ⇒ **fail run** with actionable error.
- **DoD:** Misconfigured runs stop early with clear message; CI test covers this.

---

## EPIC F — Retrieval telemetry sanity
**Rationale:** Need internal consistency to trust hit rates.

### F1. Record retrieval parameters & compute rates
- **Files:** `hippo_mem/*/retrieval.py`, `scripts/reporting/aggregate.py`
- **Data:** `k`, `batch_size`, `requests`, `hits_at_k`, `hit_rate_at_k`.
- **Checks:** Assert `hits_at_k <= k*requests`.
- **DoD:** Report page shows a table with these fields; violations hard‑fail CI.

---

## EPIC G — Report hygiene & lineage
**Rationale:** Numeric truncation and misleading zeros.

### G1. Formatting & missing handling
- **Files:** `scripts/reporting/*.py`, templates
- **Changes:** Render 3 decimals; show “missing” instead of 0; add ⚠️ for missing pre metrics.
- **DoD:** Visual diff on `reports/*/index.md` shows improved rendering.

### G2. Add lineage widget
- **Files:** templates
- **Fields:** `replay.samples`, `store_meta.source`, dataset profile, seeds, sizes.
- **DoD:** Per‑suite header includes lineage pill set.

---

## EPIC H — Tests & CI
**Rationale:** Prevent regressions and ensure credibility.

### H1. End‑to‑end smoke
- **Files:** `tests/test_end2end_smoke.py`
- **Spec:** Run small suite (n=5) across *baseline* and one *memory* variant with `replay.samples=1`.
- **Asserts:** non‑NaN pre metrics; stores exist and are non‑stub; gating counters increase.

### H2. Spatial normalization tests
- **Files:** `tests/test_spatial_metrics.py`
- **Spec:** Round‑trip verbose path → canonical target produces EM=1 on curated cases.

### H3. Report aggregation tests
- **Files:** `tests/test_reporting.py`
- **Spec:** Missing `pre_*` displayed as “missing” and flagged; averages computed from available values only.

---

## EPIC I — Documentation updates
**Rationale:** Protocol must prevent misconfigured runs.

### I1. Update `EVAL_PROTOCOL.md`
- **Add:** Pre‑flight checklist:
  - `pre-metrics=true`,
  - `replay.samples>=1` for memory runs,
  - selected `dataset_profile`,
  - expected baseline EM range per suite.
- **Add:** Troubleshooting for gating counters = 0, spatial format mismatch.
- **DoD:** Running the protocol verbatim yields non‑stub stores and valid metrics.

### I2. Update `EVAL_PLAN.md`
- **Add:** Decision criteria for “meaningful” runs (baseline range, lineage fields present).
- **Add:** Stop‑go gate: abort if criteria unmet.

### I3. Update `README.md`
- **Add:** Short “Quick sanity run” with size=5 example and expected outputs.

---

## EPIC J — Config defaults & ablations
**Rationale:** Make good defaults the easy path.

### J1. Safer defaults
- **Files:** `configs/*.yaml`
- **Changes:** `replay.samples=3`, `pre-metrics=true`, `dataset_profile=hard` (for memory evals), turn **off** `--allow-dummy-stores` by default.
- **DoD:** Fresh run uses safe defaults unless explicitly overridden.

### J2. Gate ablation
- **Files:** `configs/ablate/sgc_rss_no_gate.yaml` (example)
- **Changes:** Ensure ablation configs still compute pre metrics and record lineage.
- **DoD:** Ablation run produces comparable telemetry; reports annotate “ablation”.

---

## EPIC K — Migration & schema versioning
**Rationale:** Prior runs include dummy data and older `metrics.json` schema.

### K1. Store cleanup tool
- **Files:** `scripts/tools/mark_run_invalid.py`
- **Behavior:** Scan a run; if `store_meta.source=="stub"` or `replay.samples==0`, mark run invalid and (optionally) quarantine store files.
- **DoD:** `reports/index.md` excludes invalidated runs; tool is idempotent.

### K2. `metrics.json` schema bump
- **Files:** `schemas/metrics.schema.json`, writer modules
- **Changes:** Add `version`, `phase`, `gating`, `retrieval` sections; add back‑compat loader in reporters.
- **DoD:** Old runs still render; new fields present for new runs.

---

## EPIC L — Step 9 (consolidation uplift test) robustness
**Rationale:** Current thresholding fails early and conflates issues.

### L1. Make uplift test dependent on valid lineage
- **Files:** `scripts/test_consolidation.py`
- **Changes:**
  1. Before checking uplift, assert lineage: `pre_*` present, `replay.samples>=1`, stores non‑stub.
  2. Parameterize threshold via CLI `--min-em-uplift` and default per suite (episodic vs semantic).
- **DoD:** Test fails with actionable lineage error if preconditions unmet; passes on smoke with small positive uplift.

---

## How to Validate (quick commands)
```bash
# 1) Smoke baseline (pre metrics on)
python -m hippo_mem.eval.harness suite=semantic preset=baselines/core n=5 seed=1337 compute.pre_metrics=true

# 2) Memory with replay (and lineage)
python -m hippo_mem.eval.harness suite=semantic preset=memory/sgc_rss n=5 seed=1337 replay.samples=1 persist=true

# 3) Build report
python scripts/reporting/aggregate.py --run runs/${RUN_ID}

# 4) Spatial canonicalization test
pytest -q tests/test_spatial_metrics.py

# 5) End‑to‑end smoke
pytest -q tests/test_end2end_smoke.py
```

---

## Checklist for PR review
- [ ] Pre‑phase per‑item metrics present and reasonable (no NaNs).
- [ ] Stores only persisted when `replay.samples>=1`; `store_meta.json` present.
- [ ] Spatial predictions/answers aligned to canonical format; tests green.
- [ ] Gating and retrieval telemetry populated; invariants enforced.
- [ ] Reports render cleanly (3‑decimals, “missing” handling, lineage pills).
- [ ] Protocol/Plan/README updated with pre‑flight and troubleshooting.
- [ ] CI: smoke, spatial, reporting tests pass.

