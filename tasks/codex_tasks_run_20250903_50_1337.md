
# Codex Task Pack — Fix Implementation & Evaluation Pipeline
_Target repo: **hippo-llm-memory-main** (paths below are relative to repo root)_  
_Context: Run `runs/run_20250903_50_1337/` shows `gating.*.attempts == 0`, empty/degenerate stores, and saturated baselines on some benches. We must correct both **code** and **tests**, not just validation._

**Why this pack exists (evidence snapshot):**
- Router idle: `gating.*.attempts == 0` in `metrics.json` across memory suites.
- Stores: `sgc_rss` and `smpd` size `0`; `hei_nw` has ~53 entries but degenerate keys (few unique vectors; many constant; `tokens_span=[0,0]`).
- Baselines saturate: `semantic` and `episodic_cross` EM ~0.98–1.00 → little headroom for uplift.
- Preflight halts after step 4.3 with “gate.attempts == 0 in dry-run”.

---

## 0) Preconditions and local smoke harness
**Files:** `scripts/ci_smoke_eval.sh`, `scripts/smoke_n50.sh`

**What to do**
1. Ensure these scripts accept a `RUN_ID` slug and **do not transform it**. Pass-through to Python via env and `--run-id` flags.
2. Add a convenience that runs TEACH→TEST for a single suite/algo (episodic as default).

**Acceptance**
```bash
RUN_ID=debug_50_1337 bash scripts/ci_smoke_eval.sh
RUN_ID=debug_50_1337 bash scripts/smoke_n50.sh
# Both complete without munging RUN_ID; produce runs/${RUN_ID}/...
```

---

## 1) Unblock routing and record gate attempts (teach & test)
**Files:** `hippo_mem/eval/harness.py`, `hippo_mem/common/gates.py` (types only), `hippo_mem/episodic/gating.py`, `hippo_mem/relational/gating.py`, `hippo_mem/spatial/gating.py`

**Goal**
- Ensure gate objects are actually invoked and **attempt counters increment** in both TEACH and TEST phases.
- Write `metrics["gating"][<kind>] = {"attempts": N, "accepted": M, "skipped": K}`.

**Implementation hints**
- In `harness.evaluate(...)`, when `modules` include a memory kind, call its gate:
  ```python
  decision = gate.decide(context=ctx, hidden=hidden, reward=reward)
  gating_counters[kind]["attempts"] += 1
  if decision.action == "insert": gating_counters[kind]["accepted"] += 1
  else: gating_counters[kind]["skipped"] += 1
  ```
- In TEST, even when the gate decides “skip insert”, retrieval should still be attempted where applicable (separate read path).

**Acceptance**
- Run a tiny TEACH: attempts > 0 and accepted ≥ 1 for episodic.
- In TEST, `gating.episodic.attempts > 0` and metrics.json contains the structure above.

---

## 2) Implement explicit TEACH → TEST handoff
**Files:** `scripts/eval_model.py`, `hippo_mem/eval/harness.py`, `scripts/run_memory.py`, `scripts/store_paths.py`

**Goal**
- A separate TEACH pass writes stores, then TEST uses them.
- Persist and resolve `store_dir` & `session_id` consistently across both phases using **the same RUN_ID**.

**Implementation hints**
- `eval_model.py` already mentions modes; add CLI flags:
  - `--mode teach|replay|test`
  - When `--mode teach`, set `cfg.memory.mode="teach"` and skip evaluation aggregation except write counters and store size.
- In TEST, assert store dir exists and load stores by `{store_dir}/{algo}/{session_id}`.

**Acceptance**
```bash
RUN_ID=demo_n50 python scripts/eval_model.py --mode teach +suite=episodic n=3 seed=1337
# creates runs/${RUN_ID}/stores/<algo>/<session_id>/... and metrics with inserted>0
RUN_ID=demo_n50 python scripts/eval_model.py --mode test +suite=episodic n=3 seed=1337
# uses the same store and produces gating attempts >0 and memory retrieval calls >0
```

---

## 3) Fix RUN_ID: keep it simple and unchanged everywhere
**Files:** `hippo_mem/utils/ids.py`, `hippo_mem/utils/stores.py`, `scripts/store_paths.py`, `tests/test_preflight.py`

**Goal**
- Accept only slugs via `validate_run_id()`; **do not** inject dates or strip characters.
- Ensure store/session IDs resolve to `runs/${RUN_ID}/stores/<algo>/<session_id>/...` without extra munging.

**Implementation hints**
- `ids.py` already validates. Audit all usages and remove transformations.
- In `stores.py` and `store_paths.py`, derive `session_id` as `f"{algo[:3]}_{RUN_ID}_n{n}_seed{seed}"` (or keep existing convention) — but **never** alter `RUN_ID`.

**Acceptance**
- `tests/test_preflight_missing_baselines_lists_both_paths` should build `expect = f"runs/{rid}/baselines/metrics.csv"` with `rid` unmodified and pass.
- Manual: `RUN_ID="my_run-50_1337"` → paths use it verbatim.

---

## 4) HEI‑NW: write real episodic keys and token spans
**Files:** `hippo_mem/episodic/store.py`, `hippo_mem/episodic/index.py`, `hippo_mem/episodic/retrieval.py`, `hippo_mem/adapters/episodic_adapter.py`

**Problems seen**
- Degenerate embeddings (few unique, near-constant vectors).  
- `tokens_span=[0,0]` in stored traces → no provenance.  
- `store_source="replay"` even in TEST implies TEACH wasn’t used.

**What to do**
1. **Embeddings**: compute key vectors from encoder hidden states (e.g., CLS or mean over `tokens_span`) and **L2 normalize** before insert.
2. **Token spans**: capture start/end from the generator or alignment; store them per trace.
3. **Metadata**: persist `trace_id`, `sample_id`, and `suite` to align writes with evaluations.
4. **Retrieval**: use FAISS or cosine search over normalized keys; pack top‑k into `MemoryTokens` with a projection layer if dims mismatch.

**Acceptance**
- After TEACH on `episodic@n=50`, `store.size >= 50` and `unique_key_vectors/size ≥ 0.8`.
- TEST shows `memory_hit_rate ≥ 0.3` (new metric below) on episodic.
- `audit_sample.jsonl` entries contain non‑trivial `tokens_span` and `trace_id` fields.

---

## 5) SGC‑RSS: persist a usable semantic KG
**Files:** `hippo_mem/relational/kg.py`, `hippo_mem/relational/tuples.py`, `hippo_mem/relational/retrieval.py`, `hippo_mem/adapters/relational_adapter.py`

**What to do**
- Parse tuples (subject, relation, object) with types; add nodes/edges to a small in‑memory graph persisted as JSONL.
- Track counters: `nodes_added`, `edges_added`, `coref_merges`.
- Retrieval should fetch the k‑hop ego graph for mention spans and pack into adapter as `MemoryTokens` (e.g., text summaries or key embeddings).

**Acceptance**
- TEACH on `semantic@n=50`: `nodes_added ≥ 100`, `edges_added ≥ 150`.
- TEST: `gating.relational.attempts > 0`, and `semantic` suite on a **hard** split (see Task 8) shows non‑saturated EM with measurable deltas vs `longctx` baseline.

---

## 6) SMPD: implement spatial writes and basic retrieval
**Files:** `hippo_mem/spatial/map.py`, `hippo_mem/spatial/place_graph.py`, `hippo_mem/spatial/retrieval.py`, `hippo_mem/adapters/spatial_adapter.py`

**What to do**
- Persist landmarks with normalized coordinates and adjacency (`room`, `door`, `object`).
- On TEACH, write nodes/edges per scene; on TEST, retrieve landmarks for current query and emit a small action hint via adapter.
- Add counters: `landmarks_added`, `edges_added`.

**Acceptance**
- `spatial` suite no longer has EM=0 across the board; require either `EM ≥ 0.10` **or** `steps_to_goal ↓ by ≥20%` vs baseline on the same seed/n.
- `gating.spatial.attempts > 0` and `accepted ≥ 1` in TEACH.

---

## 7) Strengthen preflight to fail early on inert memory
**Files:** `hippo_mem/eval/harness.py`, `scripts/eval_model.py`, `scripts/validate_store.py`, `tests/test_preflight_gate.py`

**Rules to enforce**
- If running TEST for a memory preset, **fail** when (a) `store.size==0` after TEACH expected to write; **or** (b) `sum(gating.*.attempts)==0`.
- Emit `failed_preflight.json` with explicit remediation commands (e.g., `python scripts/eval_model.py --mode teach ...`).

**Acceptance**
- Update tests to expect these failures and to verify the remediation string contains the exact CLI to fix it.

---

## 8) Make benchmarks informative (avoid saturation)
**Files:** `configs/datasets/hard.yaml`, `scripts/build_datasets.py`, `EVAL_PLAN.md`, `EVAL_PROTOCOL.md`

**What to change**
- For `semantic` and `episodic_cross`, introduce **hard** variants with distractors and entity overlap; reduce trivial exact‑match cases.
- Mark the old easy/saturated versions as **smoke tests** only.

**Acceptance**
- Baseline EM on `semantic(hard)` between `0.55–0.80` (not ≥0.95). Report both smoke and hard in `report.py` with a “non‑informative for uplift” label for saturated benches.

---

## 9) New KPIs and telemetry
**Files:** `hippo_mem/eval/harness.py`, `hippo_mem/reporting/report.py`, `scripts/report.py`

**Add**
- `memory_hit_rate` per suite: fraction with any memory read used.
- `latency_ms_delta`: difference when memory is engaged vs not (per sample; mean in metrics).
- `uplift_vs_longctx`: delta EM/F1 vs long context baseline.
- Extend `audit_sample.jsonl` with `router_path`, `topk_keys`, `distance`, `justification`.

**Acceptance**
- `metrics.csv` includes the new columns; `report.py` visualizes hit rate and uplift per suite/algo.

---

## 10) Fix & extend tests to reflect real memory use
**Files:** `tests/test_eval_model.py`, `tests/test_eval_dryrun.py`, `tests/test_preflight.py`, `tests/test_preflight_gate.py`

**Changes**
- Remove assertions that `attempts == 0` in memory tests; assert `attempts > 0` after TEACH/TEST.
- Add a test that TEACH writes ≥1 item and TEST performs ≥1 retrieval (`memory_hit_rate > 0`).

**Acceptance**
```bash
pytest -q
# All tests green; slow tests behind -m slow still pass on CI when enabled.
```

---

## 11) Documentation updates (protocol & success bars)
**Files:** `EVAL_PROTOCOL.md`, `EVAL_PLAN.md`, `DESIGN.md`

**What to add**
- Exact TEACH→TEST CLI recipes, environment variables, store layout sketch, and success bars:
  - `episodic`: +0.10 EM over core and ≥ longctx EM; hit‑rate ≥ 0.3.
  - `semantic`: uplift vs longctx on hard split.
  - `spatial`: EM ≥ 0.10 or ≥20% steps reduction.
- Note which benches are smoke tests and why.

**Acceptance**
- Docs compile in GitHub preview; commands copy/paste work end‑to‑end.

---

## 12) One‑shot “sanity sweep” to confirm fixes
**Files:** `scripts/summarize_runs.py`, `scripts/report.py`

**What to do**
- Add a helper to scan a given RUN_ID and print: store sizes, gate attempts/accept rates, hit rates, and EM/F1 vs baselines & longctx.
- Include a “red flag” section when any memory preset is inert or any bench is saturated.

**Acceptance**
```bash
RUN_ID=fix_50_1337 python scripts/summarize_runs.py --run-id $RUN_ID
# Outputs a table with gate attempts >0, store.size>0, and non-saturated benches
```

---

## Patches (sketches) to guide implementation

### A) Gate counter skeleton in harness
```python
# hippo_mem/eval/harness.py
gating = {k: {"attempts": 0, "accepted": 0, "skipped": 0} for k in ("episodic", "relational", "spatial")}

def _maybe_insert(kind, gate, ctx, store, payload):
    d = gate.decide(context=ctx, hidden=payload.get("hidden"))
    gating[kind]["attempts"] += 1
    if d.action == "insert":
        store.insert(payload)
        gating[kind]["accepted"] += 1
    else:
        gating[kind]["skipped"] += 1
```

### B) Retrieval packing
```python
# hippo_mem/episodic/retrieval.py
def retrieve(store, query_vec, k=4):
    idxs, scores = store.search(query_vec, topk=k)  # cosine / faiss
    toks = make_tokens_from_indices(store, idxs)    # shape [k, T, H]
    meta = {"indices": idxs, "scores": scores}
    return MemoryTokens(tokens=toks, mask=(scores > 0), meta=meta)
```

### C) Preflight guard
```python
# hippo_mem/eval/harness.py
if preflight:
    if mode == "test" and sum(d["attempts"] for d in gating.values()) == 0:
        _fail("gate.attempts == 0 in test", remedy="python scripts/eval_model.py --mode teach ...")
    if expect_store and metrics["store"]["size"] == 0:
        _fail("expected non-empty store after teach", remedy="python scripts/eval_model.py --mode teach ...")
```

---

## Quick‑start verification after implementation
```bash
# 1) Teach
RUN_ID=fix_50_1337 python scripts/eval_model.py --mode teach +preset=baselines/core +suite=episodic n=50 seed=1337

# 2) Test with memory
RUN_ID=fix_50_1337 python scripts/run_memory.py --suite episodic --episodic-gate on --overrides +preset=baselines/core n=50 seed=1337

# 3) Summarize
RUN_ID=fix_50_1337 python scripts/summarize_runs.py --run-id $RUN_ID
```

---

## Out-of-scope (leave as-is for now)
- Full FAISS GPU indexing — keep CPU for determinism in CI.
- LoRA training loops — just ensure adapters load if present.
- Any architectural refactor beyond what’s necessary to log attempts, write stores, and retrieve.
