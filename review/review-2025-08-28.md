# Milestone 9 — Run Review for 20250827\_1506

*Date: 2025-08-28*

This document reviews the results under `runs/20250827_1506` against **EVAL\_PLAN.md** and the goals for **HEI-NW** (episodic), **SGC-RSS** (semantic/relational), and **SMPD** (spatial/procedural). It answers: (1) whether the data is sufficient to evaluate the algorithms, (2) what additional features are needed in the pipeline, and (3) whether we can conclude anything about improvements.

---

## Executive summary

**Short answer:** the current data is **insufficient** to validate the intended memory benefits. Replay wasn’t executed, gates weren’t exercised, and core semantic tasks are saturated (EM≈1.0) even without memory. We observe at most **tiny** episodic and spatial uplifts which are likely within noise and are **not attributable** to the algorithms because ablations and cross-session controls are missing.

### Snapshot (averaged across sizes/seeds; baseline = `baselines/span_short`)

* **Episodic (HEI-NW):** baseline EM≈**0.639**, F1≈**0.353** → memory EM≈**0.664**, F1≈**0.331** (**ΔEM≈+0.025**, F1↓≈0.022; latency modestly lower).
* **Semantic (SGC-RSS):** baseline EM≈**1.000**, F1≈**0.588** → memory EM≈**1.000**, F1≈**0.583** (**no EM headroom; tasks are trivial**).
* **Spatial (SMPD):** baseline EM≈**0.017**, F1≈**0.009** → memory EM≈**0.021**, F1≈**0.010** (**ΔEM≈+0.003**; negligible).

HEI-NW episodic variants exist **only** for memory runs (no baseline comparators):
`episodic_capacity` EM≈**0.944**, `episodic_cross` EM≈**1.000**, `episodic_multi` EM≈**0.623**.
Without matching baselines and ablations these cannot evidence algorithmic benefit.

---

## What’s missing versus **EVAL\_PLAN.md** (critical gaps)

1. **Replay not executed.** All `metrics.json` files show `replay.samples = 0`, and only `pre_*` fields exist. The memory presets specify `replay: cycles: 1`, but the harness reads `replay_cycles`. This key mismatch disables replay entirely.
   → **Impact:** no ΔEM/ΔF1 after 1–3 replays; no consolidation evidence.

2. **No teach→test cross-session protocol.** All runs are `mode=test, persist=false` with no `store_dir/session_id`. No ingestion occurs; relational/spatial **gate attempts = 0** everywhere.
   → **Impact:** cannot assess write-gate precision/recall, duplicate aggregation, store growth, or true cross-session recall.

3. **Semantic suite is saturated and single-hop.** Prompts contain the facts needed to answer; baselines and SGC-RSS both achieve EM≈1.0. No multi-hop composition, no contradictions, and no time-to-stabilize tracking.
   → **Impact:** no room to demonstrate schema-guided consolidation or contradiction handling.

4. **Spatial metrics are incomplete.** Current metrics include EM/F1 only. **Success rate, path sub-optimality, steps-to-solve,** and **macro reuse** are missing; output parsing is brittle.
   → **Impact:** cannot demonstrate path integration, planning improvements, or macro benefits.

5. **Baseline parity for episodic variants is missing.** `episodic_multi`, `episodic_cross`, and `episodic_capacity` were run **only** under `memory/hei_nw`.
   → **Impact:** cannot compare memory vs. baselines on the memory-dependent variants.

6. **Ablations and sensitivity sweeps absent.** No runs with `+ablate=episodic.use_gate=false`, `relational.gate.enabled=false`, or threshold sweeps.
   → **Impact:** cannot attribute any observed deltas to specific components.

---

## Can we conclude anything today?

* **HEI-NW (episodic):** Small average EM uplift (\~+0.025) with slightly lower F1 and marginally faster time. Without replay, teach/test, or ablations, this is **non-conclusive** and likely noise.
* **SGC-RSS (semantic):** **No measurable gain**; EM already 1.0 on trivial single-hop prompts.
* **SMPD (spatial):** EM remains extremely low; tiny gains (\~+0.003) are **not meaningful** without success/suboptimality metrics and macro ablations.

**Bottom line:** No credible evidence of improvement yet. The pipeline must be fixed to align with the evaluation plan.

---

## Required fixes to make the data evaluative

1. **Enable replay:** Map `replay.cycles → replay_cycles` or read both; write `post_*` metrics.
2. **Implement teach→replay→test mode:** Run `mode=teach persist=true` to ingest; then `mode=test` with `store_dir/session_id` to load without re-teaching.
3. **Add ablations:** Expose and run gates/adapter toggles and threshold sweeps; record gate telemetry.
4. **Harden semantic suite:** Generate **multi-hop** facts, optional **contradictions**, and ensure **queries exclude the facts** (so retrieval/memory is required). Track **time-to-stabilize** across replays.
5. **Add spatial KPIs:** Compute and log **success rate**, **path sub-optimality (pred/optimal)**, and **steps**; accept both coordinate and sequence outputs; add macro ablation.
6. **Run baseline comparators for episodic variants** under `baselines/span_short`.

---

## Codex task pack — concrete edits

> Paste tasks one-by-one to Codex. Each task is scoped and includes file paths and acceptance criteria.

### Task A — Fix replay cycles key

**Edit:** `hippo_mem/eval/harness.py`
**Goal:** Respect `replay.cycles` in config by mapping it to `replay_cycles` early.
**Change:** After `_apply_model_defaults(cfg)`, insert:

```python
# Normalize replay cycles key from nested config
rc = cfg.get("replay_cycles")
if rc in (None, 0):
    nested = cfg.get("replay") or {}
    try:
        cfg.replay_cycles = int(nested.get("cycles", 0))
    except Exception:
        cfg.replay_cycles = 0
```

**Accept:** Runs with `preset=memory/hei_nw` show `replay.samples > 0` and `post_*` metrics present.

### Task B — Implement teach→test protocol

**Edit:** `hippo_mem/eval/harness.py` and `scripts/eval_model.py`
**Goal:** Support `mode ∈ {teach,replay,test}`, `persist=true`, `store_dir`, `session_id`.
**Change:** In `evaluate(cfg, outdir)`, branch:

* `mode=teach`: ingest from prompts (enable gates), **skip** metric grading, **save** stores under `store_dir/session_id`, write minimal meta.
* `mode=replay`: run `cfg.replay_cycles > 0`, **save** updated stores.
* `mode=test`: **load** stores and grade; do **not** ingest.
  **Accept:** Three sequential calls produce (i) stores on disk, (ii) replay increases store edges/traces, (iii) test uses stores with no gate attempts.

### Task C — Add ablation flags and gate sweeps

**Edit:** `configs/eval/memory/*.yaml`, `Makefile`, `hippo_mem/eval/harness.py`
**Goal:** Toggle components and sweep thresholds.
**Change:** Read `+ablate.*` into `meta.json`; plumb `relational.gate.enabled`, `spatial.gate.enabled`, `episodic.use_gate`, `episodic.use_completion`; sweep thresholds via Make targets.
**Accept:** `metrics.csv` gains `gating_enabled` and `gates.*` stats differ between on/off runs.

### Task D — Strengthen semantic generator

**Edit:** `hippo_mem/eval/datasets.py::generate_semantic(...)`
**Goal:** Add `hop_depth ∈ {2,3}`, `inject_contradictions=true|false`, and ensure **query excludes facts**.
**Change:** When `require_memory=True`, omit fact sentences from `prompt` (store them only in metadata for teach). Generate both schema-fit and mismatch frames; include `facts` with `schema_fit` and `time`.
**Accept:** Baseline `core/span_short` EM < 0.6 on multi-hop; SGC-RSS improves after teach→replay→test.

### Task E — Spatial KPIs and parsing

**Edit:** `hippo_mem/eval/score.py` and `hippo_mem/eval/harness.py`
**Goal:** Add and log `success_rate`, `suboptimality_ratio`, `steps_to_goal`.
**Change:** Implement robust parsing for answers like `"LRUD"` and `"[x, y]"`. Compute optimal path via BFS from dataset metadata and compare. Add these to `metrics.json` and `metrics.csv`.
**Accept:** `metrics.json["metrics"]["spatial"]` includes the new fields; macros on/off changes steps/suboptimality.

### Task F — Baseline runs for episodic variants

**Edit:** `Makefile` and `configs/eval/baselines/span_short.yaml`
**Goal:** Ensure `episodic_multi`, `episodic_cross`, `episodic_capacity` run under baselines.
**Change:** Extend matrix in the Make target and Hydra config to include these suites.
**Accept:** `runs/<DATE>/baselines/span_short/episodic_*/*/metrics.json` exist.

### Task G — Report deltas and CI guardrails

**Edit:** `scripts/report.py`
**Goal:** When both `pre_*` and `post_*` exist, compute and display Δ columns and 95% CI across seeds.
**Change:** Detect paired fields and emit ΔEM/ΔF1; include gate duplicate rate and map growth.
**Accept:** `reports/<DATE>/index.md` shows Δ columns and gate telemetry summaries.

---

## Minimal run recipe after fixes

1. Build datasets:
   `make datasets`
2. Baselines (incl. episodic variants):
   `make eval-baselines DATE=YYYYMMDD_HHMM`
3. Memory, **teach + replay + test**:

   ```bash
   # HEI-NW
   python scripts/eval_model.py suite=episodic preset=memory/hei_nw mode=teach persist=true store_dir=runs/20250827_1506/stores session_id=s1
   python scripts/eval_model.py suite=episodic preset=memory/hei_nw mode=replay store_dir=runs/20250827_1506/stores session_id=s1 replay.cycles=3
   python scripts/eval_model.py suite=episodic preset=memory/hei_nw mode=test  store_dir=runs/20250827_1506/stores session_id=s1
   # SGC-RSS
   python scripts/eval_model.py suite=semantic preset=memory/sgc_rss ... (same three calls)
   # SMPD
   python scripts/eval_model.py suite=spatial  preset=memory/smpd   ... (teach/test; replay optional)
   ```

---

## Final verdict

The **current runs do not yet test the hypotheses** from EVAL\_PLAN.md. Apply the tasks above, rerun the grids (including baselines for episodic variants), and then we’ll have evaluable, apples-to-apples data to judge memory benefits with statistical confidence.

