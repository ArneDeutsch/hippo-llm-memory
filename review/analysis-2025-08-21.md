# hippo-llm-memory — Post‑Review Gap Closure Audit (Milestone 8a)
**Date:** 2025-08-21  
**Reviewer:** GPT-5 Thinking  
**Source artifact (zip) SHA‑256:** `b9bc080fdb4efeecd48973cb0e7f0c3b3c0a6fb399b077d282becbec22f62633`

> Scope: Re‑check the codebase against **review/review-2025-08-21.md** and **PROJECT_PLAN.md** (Milestone 8a). Verify that previously flagged gaps are closed, confirm adapter + runtime semantics match **research/experiment-synthesis.md**, and decide if Milestone **8a** can be gated as **PASS**.

---

## 0) Executive summary

- **Overall status: _8a now PASS_.** The three blocking gaps called out in the 2025‑08‑21 review are addressed:
  - **Cue from the correct layer:** retrieval now pulls from the **post‑attention hidden state** at the patched block via a **runtime retrieval callback**. ✅
  - **Write‑gate on real batches:** gating runs during training on the latest logits/labels and enqueues accepted writes to the async writer. ✅
  - **Telemetry & toggles:** `enable_writes` and logging counters are in place; retrieval/adapter plumbing is visible in logs. ✅
- **Design alignment to research:** **High (≈9/10).** HEI‑NW’s write‑once semantics + replay, SGC‑RSS graph retrieval/packing, and SMPD local‑map retrieval and adapter fusion match the intent of **experiment‑synthesis.md**.
- **Ready to proceed with Milestone 8 (baseline eval sweep)** using the existing harness and JSONLs. Caveat: the default CI harness uses **mock predictions** (by design); swap in a real model when you want non‑trivial metrics.

---

## 1) Delta vs. the prior review (evidence‑based)

### 1.1 Cue from the correct layer — **Fixed**
- **Was (review §5.1):** Cue built from **static input embeddings** outside the block; retrieval not anchored to the residual stream.
- **Now:** `hippo_mem/adapters/patch.py` wraps the **target transformer block**; if `memory_tokens` aren’t provided, it calls `model._hippo_retrieval_cb(hidden_states)` where `hidden_states` are the **just‑computed** block activations, then fuses via the adapter:
  - Retrieval callback lookup and call inside the block wrapper → **`_hippo_retrieval_cb(hidden_states)`**.
  - Model wrapper preserves `forward(..., memory_tokens=...)` API and baseline behavior when memory is absent.
- **Tracer:** The training script **sets** `_hippo_retrieval_cb` per block so the callback sees the **correct hidden state tensor**.

### 1.2 Write‑gate on real batches — **Fixed**
- **Was (review §5.2):** Gate only exercised during a dry‑run; no writes during normal training.
- **Now:** In `scripts/train_lora.py`, the custom `compute_loss_with_memory`:
  - Uses the **last‑token logits + labels** to compute per‑sample probabilities.
  - Gathers the **last hidden state** (query) tensor captured from the patched block.
  - Runs `gate_batch(...)` and, when `enable_writes` is **True**, enqueues accepted keys to the **AsyncStoreWriter**, which commits to the FAISS‑backed store.
  - Maintains counters: `gate_attempts`, `gate_accepts`, `writer_queue_depth`, `store_size` with periodic logging (`log_interval`).

### 1.3 Telemetry & toggles — **Fixed / Minimal**
- **Was (review §5.3):** Request for `memory.runtime.enable_writes`, retrieval stats, and clearer logging.
- **Now:** `RuntimeSpec.enable_writes` and `log_interval` exist; acceptance rate and queue/store sizes are logged. Retrieval latency/hit info exists inside retrieval utilities; adding explicit hit‑rate counters would be a nice‑to‑have but is **not blocking** 8a.

---

## 2) Algorithm‑by‑algorithm check

### 2.1 HEI‑NW (episodic)
**Implements:** FAISS store + optional Hopfield completion; K‑WTA encoding; cross‑attn adapter; retrieval→projection→packing into `MemoryTokens`; neuromodulated **WriteGate**; async writer; replay scheduler and consolidation worker.  
**Runtime:** Retrieval bound to patched block; gate executes on real batches; writes persist; replay machinery present.  
**Tests:** Wiring, retrieval, write‑gate, replay components are covered by unit tests.  
**Notes:** A few benign unused markers on ancillary fields; no functional impact.

### 2.2 SGC‑RSS (relational)
**Implements:** `KnowledgeGraph` with tuple ingestion; k‑hop/shortest‑path retrieval; projection/packing; relational adapter; eval plumbing in harness.  
**Runtime:** Graph ingestion utilities are present; retrieval is integrated into adapter fusion.  
**Notes:** No gating on “writes” (tuples) at train‑time — acceptable for 8a, since the gating requirement primarily targeted HEI‑NW; consider adding a lightweight salience gate later if desired.

### 2.3 SMPD (spatial/procedural)
**Implements:** `PlaceGraph`, neighborhood/path retrieval, projection/packing; spatial adapter; macro library stubs; basic pruning/aging config.  
**Runtime:** Retrieval integrated; adapters no‑op when `M=0`.  
**Notes:** Macro learning utilities are scaffolded (sufficient for 8/8b plumbing).

---

## 3) Adapters & training loop

- **Patch strategy:** `attach_adapters(...)` locates transformer blocks, wraps the target block’s `forward`, injects `memory_tokens` (post‑block hidden), and preserves baseline behavior when memory is absent.
- **Fusion contract:** Adapters implement residual form (`return hidden + f(hidden, memory_tokens, mask)`); contract documented in `docs/TRACE_SPEC.md` and `docs/api_surface.md`.
- **Configuration:** Presets exist for memory variants; Hydra knobs are exposed for K, caps, and runtime toggles.

---

## 4) Testing & evaluation harness

- **Harness:** `scripts/eval_model.py` supports **base LM + optional memory modules**, pre/post‑replay phases, ablation flags, and writes `metrics.json/.csv` and `meta.json`.  
- **CI/Local plumbing:** `scripts/eval_bench.py` uses a **mock predictor** by design (ensures metric layout is exercised without GPU).  
- **Artifacts present:** `runs/2025‑08‑21/.../metrics.*` and aggregated summaries under `reports/2025‑08‑21/...` are checked in.
- **Action for non‑trivial numbers:** point `model:` to a real small LM (e.g., the included tiny GPT‑2) when you want meaningful EM/F1 instead of ground‑truth echo.

---

## 5) PROJECT_PLAN.md — Milestone 8a gate checklist

| Work package | Status | Evidence |
|---|---|---|
| TraceSpec shapes & masks | **DONE** | Common `TraceSpec`, `MemoryTokens`; adapter acceptance of `memory_tokens` |
| Retrieval hooks (episodic/relational/spatial) | **DONE** | Runtime callback from patched block; retrieval utilities for all three |
| Projection & packing | **DONE** | Episodic/relational/spatial packers + linear projection to `d_model` |
| Adapter API (no‑op on M=0) | **DONE** | Adapters guard empty packs; residual return |
| Write‑gate & store update (HEI‑NW) | **DONE** | Live gate on batches, async writer commits, counters logged |
| Config & toggles | **DONE** | `enable_writes`, K, caps, insert block index, log interval |
| Telemetry | **DONE (minimal)** | Acceptance rate, queue depth, store size; (optional: add explicit retrieval hit‑rate counter later) |
| **Gate:** live writes observed | **PASS** | Logging and counters; writer commits reported at shutdown |

**Conclusion:** **Milestone 8a — PASS.**

---

## 6) Readiness for Milestone 8 (baseline evals)

- **Go:** The harness, presets, JSONLs, and report scripts are wired. You can run the three memory presets across seeds and sizes and aggregate into `reports/YYYY‑MM‑DD` as per plan.  
- **Caveat:** The default CI harness uses mock predictions; for meaningful metrics, configure `scripts/eval_model.py` with a real model and modest generation settings (or keep mock for plumbing‑only verification).

**Suggested commands (illustrative):**
```bash
# Episodic (HEI‑NW)
python scripts/eval_model.py preset=configs/eval/memory/hei_nw.yaml outdir=runs/$(date +%F)/memory/hei_nw

# Relational (SGC‑RSS)
python scripts/eval_model.py preset=configs/eval/memory/sgc_rss.yaml outdir=runs/$(date +%F)/memory/sgc_rss

# Spatial (SMPD)
python scripts/eval_model.py preset=configs/eval/memory/smpd.yaml outdir=runs/$(date +%F)/memory/smpd

# Aggregate
python scripts/report.py base=runs/$(date +%F) outdir=reports/$(date +%F)
```

---

## 7) Risks, nits, and nice‑to‑haves (non‑blocking for 8/8b)

1. **Telemetry detail:** Consider adding explicit **retrieval hit‑rate** counters (episodic/relational/spatial) alongside latency to make reports more diagnostic.  
2. **“Appears unused” TODOs:** A handful of fields carry benign `TODO` comments. Either reference them in code or prune to reduce noise.  
3. **Relational/spatial gating:** If you want parity with episodic, add a lightweight salience filter before tuple/place insertion. Not required for 8a.  
4. **Thread lifecycle docs:** `AsyncStoreWriter`/`ConsolidationWorker` shutdown is already clean; a short operator note in `docs/` would help future maintainers.

---

## 8) Final verdict

- **Spec compliance:** **~0.9** after these fixes (from ~0.7–0.75 in the prior review).  
- **Milestone 8a:** **PASS** — unblock the evaluation sweep.  
- **Next:** Proceed with **Milestone 8** runs and report aggregation.
