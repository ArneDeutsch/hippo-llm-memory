
# hippo-llm-memory — Milestone 8a Verification & Readiness Review
**Date:** 2025-08-21  
**Reviewer:** GPT-5 Thinking  
**Source artifact (zip) SHA-256:** `257e5417513638969c30c8eaa28e8bc9e166aec01c0cd812f2ef9fdbe20b4bf9`

> Scope: Verify that **experiment-synthesis.md** (ground truth) is faithfully reflected in **DESIGN.md**, **PROJECT_PLAN.md**, and the **implementation/tests**. Specifically confirm whether **Milestone 8a – Runtime retrieval & write‑gate plumbing** is actually complete; assess readiness to proceed to **Milestone 8 (baseline datasets & evaluation runs)**; and identify concrete, minimal fixes.

---

## 0) Executive summary

- **Design alignment to research: Good (≈8.5/10).** The three algorithms — **HEI‑NW (episodic)**, **SGC‑RSS (relational)**, **SMPD (spatial/procedural)** — are captured cleanly in `DESIGN.md`; the adapter‑first architecture and replay framing match the **experiment‑synthesis.md** intent.
- **Adapters are wired into the LM forward pass.** `hippo_mem/adapters/patch.py` adds a fusion wrapper at a chosen transformer block and injects memory tokens during `forward`. ✅
- **Runtime retrieval is implemented and used during training.** In `scripts/train_lora.py`, memory tokens are gathered each batch and passed to the patched block(s). ✅
- **BUT: Write‑gate plumbing is only exercised in `dry_run`.** The salience gate + async writer exists and is tested, yet **is not called on real batches**; gate decisions and episodic writes are skipped in normal training. ❌
- **Cue source mismatch vs spec.** Retrieval cues are built from **token embeddings** (`get_input_embeddings()(input_ids)`) rather than the **current hidden state after block N** as specified in **experiment‑synthesis.md**. This is a material spec deviation and likely harms retrieval quality. ❌
- **Evaluation harness is in place (CI‑grade) and datasets exist,** with many prebuilt JSONLs under `data/` and scripted runs under `scripts/eval_bench.py` and `scripts/eval_model.py`. However, the committed runs in `runs/20250819/...` are **baseline stubs** (mock predictions = answers). Memory‑variant runs using a real model are not present. ⚠️

**Verdict:** **Milestone 8a is *not* fully complete.** Retrieval & adapter I/O are done, but **(1)** the cue location is incorrect (must be post‑block hidden), and **(2)** the **write‑gate → async writer → store** path is not active during real training. Fix both, then proceed to Milestone 8.


---

## 1) Research ⇄ Design consistency

**Ground truth:** `research/experiment-synthesis.md`  
**Derived:** `DESIGN.md`, `PROJECT_PLAN.md`, `EVAL_PLAN.md`

### 1.1 What matches well
- **Adapters after block N (episodic/relational/spatial)** with **LoRA knobs**, **GQA/MQA** and optional **FlashAttention** are clearly reflected (`hippo_mem/episodic/adapter.py`, `hippo_mem/relational/adapter.py`, `hippo_mem/spatial/adapter.py`; fused in `hippo_mem/adapters/patch.py`).  
- **Replay** via a CA2‑style scheduler, plus a worker for consolidation: `hippo_mem/episodic/replay.py`, `hippo_mem/consolidation/worker.py`.  
- **Schema fast‑track (SGC‑RSS)** via relational ingestion and k‑hop retrieval (`hippo_mem/relational/extract.py`, `hippo_mem/relational/kg.py`, `hippo_mem/relational/retrieval.py`) and the ingest hook in training (`ingest_training_text`).  
- **Spatial procedural (SMPD)** with place graph, path‑integration toggle, macro library and a cross‑attn adapter (`hippo_mem/spatial/*`).  
- **TRACE_SPEC**: a unified I/O contract (`docs/TRACE_SPEC.md`) and tests ensure shapes & masks are consistent.

### 1.2 What diverges or is incomplete
- **Cue location for retrieval (HEI‑NW)** — the spec says: *“Build query from current hidden state (post‑attention residual stream) at the insertion block.”*  
  **Implementation:** retrieves using **static input embeddings** in `train_lora.py` and passes prebuilt `memory_tokens` to the block. **Impact:** no context/position info; weaker recall and gating signals.  
- **Neuromodulated write‑gate (S = α·surprise + β·novelty + γ·reward, with ‘pin’ override)** — code implements the math (`hippo_mem/episodic/gating.py`), but **is not invoked on real batches** to enqueue episodes via `AsyncStoreWriter`. This violates the **experiment‑synthesis** write once semantics.
- **Evaluation realism** — CI harness (`scripts/eval_bench.py`) intentionally uses oracle predictions; acceptable for plumbing, but you still need the real `eval_model.py` path exercised on a base LM + adapters to satisfy the evaluation plan.

---

## 2) Algorithm‑by‑algorithm status

### 2.1 HEI‑NW — Episodic memory
**Present:** AssocStore + FAISS index, optional Hopfield completion, write gate, replay queue/scheduler, cross‑attn adapter, adapter patching, dataset mixing for replay.  
**Evidence:** `hippo_mem/episodic/{{store.py,retrieval.py,gating.py,replay.py,adapter.py}}`, `hippo_mem/adapters/patch.py`, `scripts/replay_dataset.py`, tests `tests/test_episodic_*`, `tests/test_adapter_wiring.py`.

**Gaps against spec:**
1) **Cue from the wrong layer.** Retrieval keyed off input embeddings, not hidden states at the fusion block.  
2) **Write gate unused during training.** Gate decisions only occur in `dry_run`; no `gate_batch` on live batches → no `AsyncStoreWriter.enqueue(...)` for real data.  
3) **No record of live write rates** (accept %, avg S) across training steps — telemetry exists but isn’t populated during real runs.

**Assessment:** Core components ✅; **runtime cue + gate plumbing ❌**.

### 2.2 SGC‑RSS — Relational semantic memory
**Present:** Tuple extraction, simple KG, k‑hop retrieval + packing, numpy‑level adapter, and a PyTorch wrapper used in training. `schema_fasttrack_ingest` flag is in place.  
**Gaps:** End‑to‑end ingest during training is optional and not validated with a real model; evaluation tasks (multi‑hop, contradiction) are present in generators but not run on a real LM.  
**Assessment:** Retrieval/adapter path ✅; **empirical validation pending**.

### 2.3 SMPD — Spatial/procedural
**Present:** Place graph, optional path‑integration, macro generator, spatial adapter, retrieval/packing.  
**Gaps:** Same as above — not exercised on a real LM; macro‑assisted step‑reduction not measured outside CI stub.  
**Assessment:** Module ✅; **real eval pending**.

---

## 3) Adapters & training loop

- **Patched fusion after block N:** `attach_adapters(model, MemoryFusionConfig)` wraps the target block and models’ `forward`, reading `model._hippo_memory_tokens`. ✅
- **Memory provision:** `train_lora.py` computes `MemoryTokens` per batch via `EpisodicSource/RelationalSource/SpatialSource` and `_gather_memory_tokens(...)`. ✅
- **Spec mismatch:** Because tokens are created **before** the block executes, cues use **embedding vectors**, not the **post‑block hidden states** mandated by the spec. ❌
- **LoRA:** `default_target_modules(...)` and `count_trainable_parameters(...)` ensure non‑zero trainables; tests cover both. ✅

**Actionable fix:** Move retrieval **into** the wrapped block (or pass the current `hidden_states` down to a retrieval callback) so cues are computed from the correct layer. See §5 below for a concrete patch sketch.

---

## 4) Testing & evaluation harness

- **Unit/property tests:** 35 tests cover adapter wiring, retrieval packing, gating properties, replay queue/scheduler, dataset/report plumbing, and LoRA targets. Good surface coverage. ✅
- **Eval harnesses:**  
  - `scripts/eval_bench.py` — CI‑grade, oracle predictions; generates `runs/<date>/...`. ✅  
  - `scripts/eval_model.py` — realistic structure with meta/CSV, ablations, pre/post replay, but used only in **dry_run** in tests. ⚠️
- **Datasets:** Deterministic generators with JSONLs already checked into `data/` for episodic/semantic/spatial in {{50, 200, 1000}} × seeds {{1337, 2025, 4242}}. ✅
- **Committed runs:** `runs/20250819/...` contain **baselines/core** only and are oracle‑perfect (EM=1.0). No memory‑variant runs with a real model are present. ⚠️

**Assessment:** Harness is sound; **needs a real‑model path + memory presets to be executed** to satisfy Milestone 8.

---

## 5) What’s missing for Milestone 8a (and how to fix it fast)

### 5.1 Cue from the correct layer
**Requirement (spec):** Build the retrieval cue from the **current hidden state at the insertion block** (post‑attention residual stream).  
**Current:** Cue from static token embeddings in `compute_loss_with_memory(...)`.

**Minimal patch direction:**
- In `attach_adapters(...)`, when wrapping `block.forward`, pass the **incoming `hidden_states`** to a **retrieval callback** that returns `MemoryTokens` on‑the‑fly. Example:

```python
# hippo_mem/adapters/patch.py (inside wrapped_forward)
mem = getattr(model, "_hippo_retrieval_cb", None)
memory_tokens = memory_tokens or (mem(hidden_states) if callable(mem) else None)
out = fusion(hidden_states, memory_tokens, **kwargs)
```

- In `train_lora.py`, set:

```python
def _block_retrieval_cb(hs):
    return _gather_memory_tokens(hs, cfg, sources)

model._hippo_retrieval_cb = _block_retrieval_cb
```

This preserves decoupling and makes cues **layer‑correct** without substantial refactors.

### 5.2 Activate write‑gate on real batches
**Requirement (spec):** For each training batch, compute **surprise** from next‑token probabilities and **novelty** vs keys; if `S>τ` (or `pin`), **write once** via `AsyncStoreWriter`.

**Minimal patch direction (within `compute_loss_with_memory`)**:
1. After calling the model once (teacher‑forcing), extract logits for the last token; compute `p = softmax(logits[..., -1])` for the observed token to get surprise `u = -log p`.
2. Let `q = hidden_states[:, -1, :]` at the fusion block (use the callback approach above to access it); get keys from the episodic store (helper to return a dense key matrix).
3. Call `decisions, rate = gate_batch(gate, probs, queries, keys, rewards, pins, provenance)` and enqueue writes for accepted decisions with `AsyncStoreWriter.enqueue(q_i, TraceValue(...))`.
4. Log `write_accept_rate`, average `S`, queue depth, and store size periodically.

### 5.3 Telemetry & toggles
- Ensure the runtime logs include: **episodic_hit_rate**, **retrieval_latency_ms**, **gate_accept_rate**, **writes_enqueued/committed**, **store_size**, and **replay mix ratio**.  
- Add a single Hydra flag `memory.runtime.enable_writes=true|false` to disable gating during ablations.

---

## 6) Are we ready for Milestone 8 (baseline evals)?

**Answer:** **Not yet.** After implementing §5.1–§5.3:

1) Run **memory presets** with a small real model (e.g., Qwen2‑1.5B or TinyLlama) on the prebuilt JSONLs:  
   ```bash
   # Episodic
   python scripts/eval_model.py suite=episodic preset=memory/hei_nw n=200 seed=1337 outdir=runs/2025-08-21/memory/hei_nw/episodic/200_1337
   # Relational
   python scripts/eval_model.py suite=semantic preset=memory/sgc_rss n=200 seed=1337 outdir=runs/2025-08-21/memory/sgc_rss/semantic/200_1337
   # Spatial
   python scripts/eval_model.py suite=spatial  preset=memory/smpd   n=200 seed=1337 outdir=runs/2025-08-21/memory/smpd/spatial/200_1337
   ```
2) Add **pre/post‑replay** cycles for HEI‑NW per `EVAL_PLAN.md` and verify ΔEM/F1 improvements.  
3) Generate `reports/2025-08-21/...` via `scripts/report.py` and archive metrics & meta for reproducibility.

---

## 7) Test plan adjustments (to make tests valuable for 8a)

- Add an integration test that **mocks a block’s hidden state**, invokes the retrieval callback inside the patched block, and asserts that: (a) `episodic_retrieve_and_pack` is called with that hidden, and (b) adapter output changes vs no‑memory baseline.
- Add a test that **stubs logits** to a known probability and verifies `gate_batch` accepts/rejects and `AsyncStoreWriter.enqueue` is called during non‑dry‑run training.
- Extend `tests/test_training.py` with a configuration where `schema_fasttrack_ingest=true` and assert that `KnowledgeGraph` grows during training.

---

## 8) Quick status table for Milestone 8a

| 8a item | Status | Evidence / note |
|---|---|---|
| TraceSpec & shapes | ✅ | `docs/TRACE_SPEC.md`, tests `tests/test_specs.py` |
| Retrieval hooks for Episodic/Relational/Spatial | ✅ | `hippo_mem/*/retrieval.py`, sources in `train_lora.py` |
| Projection & packing → `[B, M, d_model]` + mask | ✅ | All three packers; unit tests cover shapes |
| Adapter API & patching after block N | ✅ | `hippo_mem/adapters/patch.py`, `tests/test_adapter_wiring.py` |
| **Write‑gate called on real batches** | ❌ | Present only in `dry_run` path; not in normal training |
| **Cue taken from post‑block hidden** | ❌ | Currently from input embeddings; violates spec |
| Telemetry (hit‑rate/latency/write‑rate) | ⚠️ | Retrieval logs present; write‑rate not populated in live runs |

---

## 9) Prioritised task list (ready for Codex)

1. **Move retrieval to the block hidden.**  
   - Implement `_hippo_retrieval_cb(hidden_states) -> MemoryTokens` and call it from the wrapped block (see §5.1).  
   - Remove pre‑forward embedding‑based retrieval.  
   - Add unit test to assert hidden‑based cueing.

2. **Enable write‑gate during training.**  
   - In `compute_loss_with_memory`, compute surprise from logits; build queries from block hidden; call `gate_batch`; enqueue accepted writes via `AsyncStoreWriter`.  
   - Log accept rate and store growth.  
   - Add unit test stubbing logits to ensure enqueue is called.

3. **Plumb KG ingestion & spatial traces in real runs.**  
   - If `schema_fasttrack_ingest=true`, ingest tuples from `train_ds` each epoch and verify KG size increases.  
   - Log relational hit‑rate and path sizes.

4. **Run real evaluation presets (Milestone 8).**  
   - Execute `scripts/eval_model.py` for `memory/hei_nw`, `memory/sgc_rss`, `memory/smpd` on the existing JSONLs (200‑item subset, the three seeds).  
   - Commit `metrics.json/csv` + `meta.json` artifacts under `runs/{{date}}/memory/...`.  
   - Generate `reports/{{date}}` via `scripts/report.py`.

5. **Polish (optional but quick):**  
   - Add `memory.runtime.enable_writes` flag.  
   - Ensure `PlaceGraph`/`KnowledgeGraph` have graceful shutdown in `TrainerContext.stop()`.  
   - Document the retrieval callback in `docs/api_surface.md`.

---

## 10) Verdict

- **Spec compliance:** **0.8 → 0.9** once cue location + write‑gate activation are fixed (today it’s ~**0.7–0.75**).  
- **Milestone 8a:** **FAIL (near‑complete)** — retrieval & adapters are correct; missing *live* write‑gate and *correct cue layer*.  
- **Readiness for Milestone 8:** **BLOCKED pending 8a fixes.** After §5 is implemented, proceed to baseline memory runs and report aggregation.

---

*End of review.*
