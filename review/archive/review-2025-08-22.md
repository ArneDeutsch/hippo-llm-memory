# hippo-llm-memory — Milestone 8 Verification & Readiness Review
**Date:** 2025-08-22  
**Reviewer:** GPT-5 Thinking  
**Source artifact (zip) SHA-256:** `f080d50230b35ab014eb0ed799365cec83b871ad58aea1b86a5420d464776796`

> Scope: Verify that **Milestone 8** (baselines) is complete per **MILESTONE_8_PLAN.md** and **EVAL_PLAN.md**; confirm datasets, run matrix, and reports; and assess readiness for **Milestone 9**.

---

## 0) Executive summary

- **Datasets present for all suites and splits.** `data/episodic|semantic|spatial/` each contain 9 JSONLs (sizes 50/200/1000 × seeds 1337/2025/4242). Checksums files present per suite: episodic: ✅, semantic: ✅, spatial: ✅.
- **Baseline runs executed over full matrix:** suites × presets × sizes × seeds = **81 runs**. Metrics found: **81** (expected 81) → ✅ complete.
- **Metrics & reports generated:** Each run has `metrics.json/csv` + `meta.json`. Per‑suite summaries exist under `reports/20250822/<suite>/summary.md` and match the aggregated numbers below. ✅
- **Intentional mock predictions:** Baseline harness (`scripts/eval_bench.py`) sets `pred == answer` to verify plumbing; thus **EM=1.0 everywhere** and `latency_ms≈0`. This is acceptable for Milestone 8’s *plumbing* goal but not informative for model quality comparisons. ⚠️
- **Gaps vs EVAL_PLAN compute section:** No wall‑clock runtime or memory‑footprint columns in `metrics.json`; only `compute.tokens` is recorded. Recommend adding `compute.time_ms_per_100` and `compute.rss_mb`. ⚠️
- **Meta completeness:** `meta.json` captures `git_sha`, `pip_hash`, `model`, `seed`, etc., but omits `suite`, `preset`, and `n` (present only in `metrics.json`). Recommend duplicating for standalone provenance. 🛈

**Verdict:** **Milestone 8 is complete as a reproducible baseline/plumbing milestone.** Data, run matrix, and reports are in place. To prepare for Milestone 9 comparisons, add minimal compute/memory telemetry and a top‑level `reports/20250822/index.md` roll‑up. ✅/⚠️

---

## 1) Evidence snapshots

### 1.1 Dataset audit

- **episodic**: 9 files; e.g., `1000_1337.jsonl, 1000_2025.jsonl, 1000_4242.jsonl`; checksums.json: ✅
- **semantic**: 9 files; e.g., `1000_1337.jsonl, 1000_2025.jsonl, 1000_4242.jsonl`; checksums.json: ✅
- **spatial**: 9 files; e.g., `1000_1337.jsonl, 1000_2025.jsonl, 1000_4242.jsonl`; checksums.json: ✅

### 1.2 Run‑matrix coverage (should be 3 seeds per cell)

| suite | preset | n | runs | seeds | em | tokens |
|---|---|---:|---:|---|---:|---:|
| episodic | baselines/core | 50 | 3 | [1337, 2025, 4242] | 1.000 | 636.000 |
| episodic | baselines/core | 200 | 3 | [1337, 2025, 4242] | 1.000 | 2541.333 |
| episodic | baselines/core | 1000 | 3 | [1337, 2025, 4242] | 1.000 | 12736.000 |
| episodic | baselines/longctx | 50 | 3 | [1337, 2025, 4242] | 1.000 | 636.000 |
| episodic | baselines/longctx | 200 | 3 | [1337, 2025, 4242] | 1.000 | 2541.333 |
| episodic | baselines/longctx | 1000 | 3 | [1337, 2025, 4242] | 1.000 | 12736.000 |
| episodic | baselines/rag | 50 | 3 | [1337, 2025, 4242] | 1.000 | 636.000 |
| episodic | baselines/rag | 200 | 3 | [1337, 2025, 4242] | 1.000 | 2541.333 |
| episodic | baselines/rag | 1000 | 3 | [1337, 2025, 4242] | 1.000 | 12736.000 |
| semantic | baselines/core | 50 | 3 | [1337, 2025, 4242] | 1.000 | 950.000 |
| semantic | baselines/core | 200 | 3 | [1337, 2025, 4242] | 1.000 | 3800.000 |
| semantic | baselines/core | 1000 | 3 | [1337, 2025, 4242] | 1.000 | 19000.000 |

### 1.3 Per‑suite aggregates (matches `reports/20250822/*/summary.md`)

| suite | preset | EM | tokens | runs |
|---|---|---:|---:|---:|
| episodic | baselines/core | 1.000 | 5304.444 | 9 |
| episodic | baselines/longctx | 1.000 | 5304.444 | 9 |
| episodic | baselines/rag | 1.000 | 5304.444 | 9 |
| semantic | baselines/core | 1.000 | 7916.667 | 9 |
| semantic | baselines/longctx | 1.000 | 7916.667 | 9 |
| semantic | baselines/rag | 1.000 | 7916.667 | 9 |
| spatial | baselines/core | 1.000 | 12222.556 | 9 |
| spatial | baselines/longctx | 1.000 | 12222.556 | 9 |
| spatial | baselines/rag | 1.000 | 12222.556 | 9 |

---

## 2) Conformance to **EVAL_PLAN.md**

- **Baselines defined (core / rag / longctx):** Present under `configs/eval/baselines/*.yaml` and exercised across all suites/sizes/seeds. ✅
- **Metrics:** `EM` (per suite) is recorded; `compute.tokens` present; **missing** recommended fields: wall‑clock time and memory footprint. ⚠️
- **Telemetry:** As expected for baselines with no memory, `metrics.memory` is empty and no retrieval telemetry is logged. ✅
- **Artifacts layout:** Matches plan (`runs/<date>/baselines/...`, `reports/<date>/<suite>/summary.md`). ✅
- **Determinism:** Seeds are carried through; datasets are fixed and checksummed per suite. ✅ (Consider adding a top‑level `data/checksums.json` manifest to mirror the per‑suite files.)

## 3) Script‑by‑script notes

- `scripts/run_baselines_bench.py`: Drives the full matrix; CLI mirrors plan; ✅.
- `scripts/eval_bench.py`: CI‑grade harness with **oracle predictions** by design; good for plumbing; ⚠️ for meaningful baselines.
- `scripts/eval_model.py`: Real evaluation harness scaffolding exists (supports pre/post‑replay & ablations); not used in Milestone 8 runs; will be central in Milestone 9. ✅/⏭️
- `scripts/report.py`: Produces per‑suite tables; **plots optional** if matplotlib available. Current repo contains only the Markdown tables. Suggest emitting simple PNG bar charts and a top‑level `reports/<date>/index.md`. 🛈
- `scripts/audit_datasets.py`: Verifies presence & SHA256 via per‑suite `checksums.json`. ✅

## 4) What’s missing or brittle (recommended before / during Milestone 9)

1. **Compute telemetry**: add `compute.time_ms_per_100` (CPU), `compute.rss_mb` to `metrics.json`; even for the mock pipeline this yields usable baselines for overhead plots.
2. **`meta.json` completeness**: include `suite`, `preset`, and `n` so a single file carries full provenance (helps post‑hoc audits).
3. **Latency field realism**: `metrics.csv.latency_ms` is `0.0` everywhere; measure end‑to‑end pipeline latency (even without a model) to catch regressions.
4. **Reports roll‑up**: add `reports/20250822/index.md` that links the three suite summaries and embeds optional plots.
5. **One manifest for datasets**: add `data/MANIFEST.json` summarising suite→files→sha256→counts; keep per‑suite checksums as they are.
6. **Sanity spot‑checks**: include a tiny `reports/20250822/smoke.md` with 2–3 raw rows per suite to visually confirm prompt/answer format integrity.

## 5) Milestone 9 readiness & proposed adjustments

**Goal (from PROJECT_PLAN.md):** train with each memory (HEI‑NW, SGC‑RSS, SMPD), evaluate, and run ablations; produce an aggregated report.

### 5.1 Additions to work packages (proposed)

- **Real baselines for comparison:** run `eval_model.py` with a small open model (e.g., `models/tiny-gpt2` or `TinyLlama`) *without* memory to establish non‑oracle baselines on the same matrix (50/200/1000 × 3 seeds).
- **Telemetry requirements (all memory runs):** log per‑step write‑gate accept rate, store growth (items/tokens), retrieval hit@k, and replay cycles; persist under `metrics.memory.{write,read,replay}`.
- **Compute budget columns:** record `compute.time_ms_per_100`, `compute.rss_mb`, and `compute.tokens` for all runs to enable cost/quality plots.
- **Ablations matrix:** for each memory, toggle its key knobs (e.g., HEI‑NW: gate on/off; Hopfield completion on/off; SGC‑RSS: schema fast‑track on/off; SMPD: macro distillation on/off). Keep `n=200`, seeds=3 for ablations to manage cost.
- **Combined model:** one `memory/all` run at `n=200` with 3 seeds; collect interactions and overhead.
- **Reports:** generate `reports/<date>/index.md` with side‑by‑side tables (baselines vs memory) and simple bar plots; include an *Ablations* section.

### 5.2 Proposed Gate (definition of done)

- All memory variants (`hei_nw`, `sgc_rss`, `smpd`, `all`) have **runs at sizes 50/200** with seeds **1337/2025/4242** and complete metrics (including compute & telemetry).
- **EM (or task‑specific metric) improves** over real baselines by a **meaningful margin** (suggest: ≥10 points EM for episodic; suite‑appropriate for others) on `n=200`.
- **Ablation deltas** are present and interpretable (gate off vs on, etc.).
- `reports/<date>/index.md` summarises results with tables + plots and links to per‑suite details.

### 5.3 Suggested diff for `PROJECT_PLAN.md`

```diff
 # Milestone 9 – Memory‑augmented training, evaluation & ablations
-**Work packages**
-1. [ ] HEI‑NW evaluation: fine-tune the model ...
-2. [ ] SGC-RSS evaluation: train with the relational module ...
-3. [ ] SMPD evaluation: train with the spatial module ...
-4. [ ] Combined model ...
-5. [ ] Ablation study ...
+**Work packages**
+0. [ ] **Establish real (non‑oracle) baselines** with `scripts/eval_model.py` (no memory) over the same matrix (sizes 50/200/1000 × seeds 1337/2025/4242).
+1. [ ] HEI‑NW evaluation: fine‑tune with episodic memory; log gate accept %, store growth, retrieval hit@k, replay cycles; record compute (tokens, time_ms_per_100, rss_mb).
+2. [ ] SGC‑RSS evaluation: train with relational memory; log schema fast‑track rate and contradiction filter stats; record compute columns.
+3. [ ] SMPD evaluation: train with spatial memory; log local‑map size and steps‑to‑solve; record compute columns.
+4. [ ] Combined model: enable all memories; run at n=200 with 3 seeds; collect trade‑offs.
+5. [ ] Ablations: for each memory, toggle its key knobs (gate on/off, completion on/off, fast‑track on/off, macro distillation on/off) at n=200 × 3 seeds.
+6. [ ] Reports: create `reports/<date>/index.md` aggregating tables and plots across baselines and memory variants.

-**Gate**
-- Experiments are logged under runs/YYYYMMDD/...; metrics aggregated into a report.
+**Gate**
+- For each memory variant and baselines (real), runs exist at sizes 50 and 200 with seeds 1337/2025/4242 and include compute & telemetry fields.
+- Improvements over real baselines are demonstrated on n=200 (episodic EM +10 points suggested; others suite‑specific).
+- Ablation effects are clear (directional, non‑noisy).
+- `reports/<date>/index.md` is present with tables/plots and links to per‑suite summaries.
```


---

## 6) Conclusion

Milestone 8’s objectives are **met** as a plumbing & reproducibility milestone: data generation and integrity are in place; the full baseline run matrix was executed; and per‑suite reports were written. To make the upcoming Milestone 9 comparisons meaningful, add minimal compute telemetry, a report roll‑up, and establish *real* baselines using `scripts/eval_model.py`. With those in place, you’re ready to flip on memory modules and run the ablations.
