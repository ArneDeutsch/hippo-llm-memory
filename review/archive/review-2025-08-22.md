# hippo-llm-memory â€” Milestone 8 Verification & Readiness Review
**Date:** 2025-08-22  
**Reviewer:** GPT-5 Thinking  
**Source artifact (zip) SHA-256:** `f080d50230b35ab014eb0ed799365cec83b871ad58aea1b86a5420d464776796`

> Scope: Verify that **Milestone 8** (baselines) is complete per **MILESTONE_8_PLAN.md** and **EVAL_PLAN.md**; confirm datasets, run matrix, and reports; and assess readiness for **Milestone 9**.

---

## 0) Executive summary

- **Datasets present for all suites and splits.** `data/episodic|semantic|spatial/` each contain 9 JSONLs (sizes 50/200/1000 Ã— seeds 1337/2025/4242). Checksums files present per suite: episodic: âœ…, semantic: âœ…, spatial: âœ….
- **Baseline runs executed over full matrix:** suites Ã— presets Ã— sizes Ã— seeds = **81 runs**. Metrics found: **81** (expected 81) â†’ âœ… complete.
- **Metrics & reports generated:** Each run has `metrics.json/csv` + `meta.json`. Perâ€‘suite summaries exist under `reports/20250822/<suite>/summary.md` and match the aggregated numbers below. âœ…
- **Intentional mock predictions:** Baseline harness (`scripts/eval_bench.py`) sets `pred == answer` to verify plumbing; thus **EM=1.0 everywhere** and `latency_msâ‰ˆ0`. This is acceptable for MilestoneÂ 8â€™s *plumbing* goal but not informative for model quality comparisons. âš ï¸
- **Gaps vs EVAL_PLAN compute section:** No wallâ€‘clock runtime or memoryâ€‘footprint columns in `metrics.json`; only `compute.tokens` is recorded. Recommend adding `compute.time_ms_per_100` and `compute.rss_mb`. âš ï¸
- **Meta completeness:** `meta.json` captures `git_sha`, `pip_hash`, `model`, `seed`, etc., but omits `suite`, `preset`, and `n` (present only in `metrics.json`). Recommend duplicating for standalone provenance. ğŸ›ˆ

**Verdict:** **Milestone 8 is complete as a reproducible baseline/plumbing milestone.** Data, run matrix, and reports are in place. To prepare for MilestoneÂ 9 comparisons, add minimal compute/memory telemetry and a topâ€‘level `reports/20250822/index.md` rollâ€‘up. âœ…/âš ï¸

---

## 1) Evidence snapshots

### 1.1 Dataset audit

- **episodic**: 9 files; e.g., `1000_1337.jsonl, 1000_2025.jsonl, 1000_4242.jsonl`; checksums.json: âœ…
- **semantic**: 9 files; e.g., `1000_1337.jsonl, 1000_2025.jsonl, 1000_4242.jsonl`; checksums.json: âœ…
- **spatial**: 9 files; e.g., `1000_1337.jsonl, 1000_2025.jsonl, 1000_4242.jsonl`; checksums.json: âœ…

### 1.2 Runâ€‘matrix coverage (should be 3 seeds per cell)

| suite | preset | n | runs | seeds | em | tokens |
|---|---|---:|---:|---|---:|---:|
| episodic | baselines/core | 50 | 3 | [1337, 2025, 4242] | 1.000 | 636.000 |
| episodic | baselines/core | 200 | 3 | [1337, 2025, 4242] | 1.000 | 2541.333 |
| episodic | baselines/core | 1000 | 3 | [1337, 2025, 4242] | 1.000 | 12736.000 |
| episodic | baselines/longctx | 50 | 3 | [1337, 2025, 4242] | 1.000 | 636.000 |
| episodic | baselines/longctx | 200 | 3 | [1337, 2025, 4242] | 1.000 | 2541.333 |
| episodic | baselines/longctx | 1000 | 3 | [1337, 2025, 4242] | 1.000 | 12736.000 |
| episodic | baselines/rag | 50 | 3 | [1337, 2025, 4242] | 1.000 | 636.000 |
| episodic | baselines/rag | 200 | 3 | [1337, 2025, 4242] | 1.000 | 2541.333 |
| episodic | baselines/rag | 1000 | 3 | [1337, 2025, 4242] | 1.000 | 12736.000 |
| semantic | baselines/core | 50 | 3 | [1337, 2025, 4242] | 1.000 | 950.000 |
| semantic | baselines/core | 200 | 3 | [1337, 2025, 4242] | 1.000 | 3800.000 |
| semantic | baselines/core | 1000 | 3 | [1337, 2025, 4242] | 1.000 | 19000.000 |

### 1.3 Perâ€‘suite aggregates (matches `reports/20250822/*/summary.md`)

| suite | preset | EM | tokens | runs |
|---|---|---:|---:|---:|
| episodic | baselines/core | 1.000 | 5304.444 | 9 |
| episodic | baselines/longctx | 1.000 | 5304.444 | 9 |
| episodic | baselines/rag | 1.000 | 5304.444 | 9 |
| semantic | baselines/core | 1.000 | 7916.667 | 9 |
| semantic | baselines/longctx | 1.000 | 7916.667 | 9 |
| semantic | baselines/rag | 1.000 | 7916.667 | 9 |
| spatial | baselines/core | 1.000 | 12222.556 | 9 |
| spatial | baselines/longctx | 1.000 | 12222.556 | 9 |
| spatial | baselines/rag | 1.000 | 12222.556 | 9 |

---

## 2) Conformance to **EVAL_PLAN.md**

- **Baselines defined (core / rag / longctx):** Present under `configs/eval/baselines/*.yaml` and exercised across all suites/sizes/seeds. âœ…
- **Metrics:** `EM` (per suite) is recorded; `compute.tokens` present; **missing** recommended fields: wallâ€‘clock time and memory footprint. âš ï¸
- **Telemetry:** As expected for baselines with no memory, `metrics.memory` is empty and no retrieval telemetry is logged. âœ…
- **Artifacts layout:** Matches plan (`runs/<date>/baselines/...`, `reports/<date>/<suite>/summary.md`). âœ…
- **Determinism:** Seeds are carried through; datasets are fixed and checksummed per suite. âœ… (Consider adding a topâ€‘level `data/checksums.json` manifest to mirror the perâ€‘suite files.)

## 3) Scriptâ€‘byâ€‘script notes

- `scripts/run_baselines_bench.py`: Drives the full matrix; CLI mirrors plan; âœ….
- `scripts/eval_bench.py`: CIâ€‘grade harness with **oracle predictions** by design; good for plumbing; âš ï¸ for meaningful baselines.
- `scripts/eval_model.py`: Real evaluation harness scaffolding exists (supports pre/postâ€‘replay & ablations); not used in MilestoneÂ 8 runs; will be central in MilestoneÂ 9. âœ…/â­ï¸
- `scripts/report.py`: Produces perâ€‘suite tables; **plots optional** if matplotlib available. Current repo contains only the Markdown tables. Suggest emitting simple PNG bar charts and a topâ€‘level `reports/<date>/index.md`. ğŸ›ˆ
- `scripts/audit_datasets.py`: Verifies presence & SHA256 via perâ€‘suite `checksums.json`. âœ…

## 4) Whatâ€™s missing or brittle (recommended before / during MilestoneÂ 9)

1. **Compute telemetry**: add `compute.time_ms_per_100` (CPU), `compute.rss_mb` to `metrics.json`; even for the mock pipeline this yields usable baselines for overhead plots.
2. **`meta.json` completeness**: include `suite`, `preset`, and `n` so a single file carries full provenance (helps postâ€‘hoc audits).
3. **Latency field realism**: `metrics.csv.latency_ms` is `0.0` everywhere; measure endâ€‘toâ€‘end pipeline latency (even without a model) to catch regressions.
4. **Reports rollâ€‘up**: add `reports/20250822/index.md` that links the three suite summaries and embeds optional plots.
5. **One manifest for datasets**: add `data/MANIFEST.json` summarising suiteâ†’filesâ†’sha256â†’counts; keep perâ€‘suite checksums as they are.
6. **Sanity spotâ€‘checks**: include a tiny `reports/20250822/smoke.md` with 2â€“3 raw rows per suite to visually confirm prompt/answer format integrity.

## 5) MilestoneÂ 9 readiness & proposed adjustments

**Goal (from PROJECT_PLAN.md):** train with each memory (HEIâ€‘NW, SGCâ€‘RSS, SMPD), evaluate, and run ablations; produce an aggregated report.

### 5.1 Additions to work packages (proposed)

- **Real baselines for comparison:** run `eval_model.py` with a small open model (e.g., `models/tiny-gpt2` or `TinyLlama`) *without* memory to establish nonâ€‘oracle baselines on the same matrix (50/200/1000 Ã— 3 seeds).
- **Telemetry requirements (all memory runs):** log perâ€‘step writeâ€‘gate accept rate, store growth (items/tokens), retrieval hit@k, and replay cycles; persist under `metrics.memory.{write,read,replay}`.
- **Compute budget columns:** record `compute.time_ms_per_100`, `compute.rss_mb`, and `compute.tokens` for all runs to enable cost/quality plots.
- **Ablations matrix:** for each memory, toggle its key knobs (e.g., HEIâ€‘NW: gate on/off; Hopfield completion on/off; SGCâ€‘RSS: schema fastâ€‘track on/off; SMPD: macro distillation on/off). Keep `n=200`, seeds=3 for ablations to manage cost.
- **Combined model:** one `memory/all` run at `n=200` with 3 seeds; collect interactions and overhead.
- **Reports:** generate `reports/<date>/index.md` with sideâ€‘byâ€‘side tables (baselines vs memory) and simple bar plots; include an *Ablations* section.

### 5.2 Proposed Gate (definition of done)

- All memory variants (`hei_nw`, `sgc_rss`, `smpd`, `all`) have **runs at sizes 50/200** with seeds **1337/2025/4242** and complete metrics (including compute & telemetry).
- **EM (or taskâ€‘specific metric) improves** over real baselines by a **meaningful margin** (suggest: â‰¥10 points EM for episodic; suiteâ€‘appropriate for others) on `n=200`.
- **Ablation deltas** are present and interpretable (gate off vs on, etc.).
- `reports/<date>/index.md` summarises results with tables + plots and links to perâ€‘suite details.

### 5.3 Suggested diff for `PROJECT_PLAN.md`

```diff
 # Milestone 9 â€“ Memoryâ€‘augmented training, evaluation & ablations
-**Work packages**
-1. [ ] HEIâ€‘NW evaluation: fine-tune the model ...
-2. [ ] SGC-RSS evaluation: train with the relational module ...
-3. [ ] SMPD evaluation: train with the spatial module ...
-4. [ ] Combined model ...
-5. [ ] Ablation study ...
+**Work packages**
+0. [ ] **Establish real (nonâ€‘oracle) baselines** with `scripts/eval_model.py` (no memory) over the same matrix (sizes 50/200/1000 Ã— seeds 1337/2025/4242).
+1. [ ] HEIâ€‘NW evaluation: fineâ€‘tune with episodic memory; log gate acceptÂ %, store growth, retrieval hit@k, replay cycles; record compute (tokens, time_ms_per_100, rss_mb).
+2. [ ] SGCâ€‘RSS evaluation: train with relational memory; log schema fastâ€‘track rate and contradiction filter stats; record compute columns.
+3. [ ] SMPD evaluation: train with spatial memory; log localâ€‘map size and stepsâ€‘toâ€‘solve; record compute columns.
+4. [ ] Combined model: enable all memories; run at n=200 with 3 seeds; collect tradeâ€‘offs.
+5. [ ] Ablations: for each memory, toggle its key knobs (gate on/off, completion on/off, fastâ€‘track on/off, macro distillation on/off) at n=200 Ã— 3 seeds.
+6. [ ] Reports: create `reports/<date>/index.md` aggregating tables and plots across baselines and memory variants.

-**Gate**
-- Experiments are logged under runs/YYYYMMDD/...; metrics aggregated into a report.
+**Gate**
+- For each memory variant and baselines (real), runs exist at sizes 50 and 200 with seeds 1337/2025/4242 and include compute & telemetry fields.
+- Improvements over real baselines are demonstrated on n=200 (episodic EM +10 points suggested; others suiteâ€‘specific).
+- Ablation effects are clear (directional, nonâ€‘noisy).
+- `reports/<date>/index.md` is present with tables/plots and links to perâ€‘suite summaries.
```


---

## 6) Conclusion

MilestoneÂ 8â€™s objectives are **met** as a plumbing & reproducibility milestone: data generation and integrity are in place; the full baseline run matrix was executed; and perâ€‘suite reports were written. To make the upcoming MilestoneÂ 9 comparisons meaningful, add minimal compute telemetry, a report rollâ€‘up, and establish *real* baselines using `scripts/eval_model.py`. With those in place, youâ€™re ready to flip on memory modules and run the ablations.
