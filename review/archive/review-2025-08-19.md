
# hippo-llm-memory — Implementation & Testing Review
**Date:** 2025-08-19  
**Reviewer:** GPT-5 Thinking  
**Source artifact (zip) SHA-256:** `cb80eab247970b011186c9076e76b1ece4922e47dc6d4fef55a9521a1e910c75`

> Scope: Deep review of the three algorithms (HEI‑NW, SGC‑RSS, SMPD), their implementation status, the training script (`scripts/train_lora.py`), synthetic data, and the evaluation plan/harness. Focus on “does it match the spec in `research/experiment-synthesis.md`?”, “is the testing plan sound?”, and “is the first LoRA training run credible?”.

---

## 0) Executive summary

- **Core implementations exist but are not yet wired into a real training/eval loop.** You have solid modules for episodic (store, gating, replay), semantic (KG), and spatial (place graph) memory. However, the LoRA training script currently fine-tunes a **tiny GPT‑2** on **IMDB** by default and **does not integrate the adapters into the model’s forward pass** during training; the background replay worker runs but its outputs are not fed to the trainer. This explains your observation: the run was **very fast** and unlikely to produce anything meaningful for your memory tasks.
- **Evaluation harness is a CI stub, not a scientific benchmark.** `scripts/eval_bench.py` sets predictions equal to ground truth to exercise plumbing. Unsurprisingly, reports show **EM = 1.0** for all presets. This is intentional for CI, but you do not yet have a faithful evaluation that invokes a real model plus memory adapters.
- **Synthetic data is a good start, but not yet used by the training loop.** You generated episodic/semantic/spatial JSONL datasets under `data/`, but `train_lora.py` ignores them and loads `imdb`. Data format is close to what TRL needs, but you’ll want instruction-style formatting and clear **train/val/test** splits.
- **LoRA details need tightening.** No explicit `target_modules` are set; for some architectures (esp. GPT‑2) PEFT may end up attaching LoRA to **zero parameters**, yielding a no‑op “training” that still runs quickly. There is also no assertion on “trainable parameter count > 0”.

**Bottom line:** Architecture/components match the spec *in spirit*; the missing piece is the **end-to-end plumbing** that (a) inserts the adapters into the base LM forward pass, (b) feeds memory/replay data into training, and (c) evaluates with a real model. Once those are in place, the LoRA run will be slower (as expected) and produce meaningful results.

---

## 1) Alignment to spec (by algorithm)

### 1.1 HEI‑NW (episodic)
**Spec (from `research/experiment-synthesis.md`):** DG‑like sparse keys, CA3‑style associative completion (e.g., modern Hopfield), neuromodulated write gate combining **surprise, novelty, reward, pin**, prioritized replay, adapter that cross‑attends to recalled traces.

**Implementation found:**
- **Store/index/completion:** `hippo_mem/episodic/store.py` uses FAISS for KNN and provides a Hopfield‑style `complete(...)`. Keys and values are persisted (SQLite wrapper in `episodic/db.py`). ✅
- **Write gate:** `hippo_mem/episodic/gating.py` implements `S = α·surprise + β·novelty + γ·reward + δ·pin`, with working novelty/surprise functions and a threshold `τ`. ✅
- **Replay:** `hippo_mem/episodic/replay.py` provides a priority queue & scheduler combining salience/recency/diversity. ✅
- **Adapter:** `hippo_mem/episodic/adapter.py` implements a cross‑attention module (own MHA with projections, LoRA knobs) that can fuse memory traces with hidden states. ✅

**Gaps:**
- **Not inserted into the base model.** The EpisodicAdapter is **instantiated** in `scripts/train_lora.py` but **never integrated** into the transformer forward path (no hooks or module replacement). ❌
- **Training loop doesn’t consume replay.** A `ReplayScheduler` and `ConsolidationWorker` start, but SFT training still uses `imdb` and does not sample from memory/replay. ❌

**Assessment:** Components meet the design intent; end‑to‑end integration is missing.

---

### 1.2 SGC‑RSS (semantic/relational)
**Spec:** Parse episodes into tuples; persist into a graph store; schema‑fit bias and contradiction handling; adapter fuses relational embeddings; replay feeds consolidation.

**Implementation found:**
- **Graph store:** `hippo_mem/relational/kg.py` uses NetworkX + SQLite, node embeddings, schema index hooks, maintenance (prune/rollback). ✅
- **Adapter:** `hippo_mem/relational/adapter.py` has deterministic attention‑style fusion over graph features; returns weighted sums with softmax over similarity. ✅

**Gaps:**
- **Adapter not wired into model forward.** Same as episodic. ❌
- **No semantic generator in training loop.** Data exists (`data/semantic_*.jsonl`) but is unused in `train_lora.py`. ❌

**Assessment:** Store/adapter present; missing integration and training/eval plumbing.

---

### 1.3 SMPD (spatial/procedural)
**Spec:** Spatial map (place graph), macros, cross‑attention adapter, replay-to-policy distillation.

**Implementation found:**
- **Place graph:** `hippo_mem/spatial/map.py` implements deterministic topological map with A*/Dijkstra, decay/prune maintenance. ✅
- **Adapter:** `hippo_mem/spatial/adapter.py` mirrors the cross‑attention pattern with optional LoRA and (G)QA knobs. ✅

**Gaps:**
- **Not inserted into model forward.** ❌
- **No policy distillation in the current trainer.** ❌

**Assessment:** Building blocks exist; not yet used in a training/evaluation loop.

---

## 2) Training pipeline review (`scripts/train_lora.py`)

**What it actually does today:**
- Defaults: `model_name = models/tiny-gpt2`, `dataset_name = imdb`, `max_steps = 500`, batch size 1, grad acc 4. It enables gradient checkpointing and (optionally) 4‑bit quantization.
- Creates **Episodic/Relational/Spatial adapters** and starts **stores + maintenance threads + replay worker**, but **trainer still consumes HF `imdb`** via TRL’s `SFTTrainer`.
- LoRA config is created without explicit `target_modules`.

**Why your run was “too fast”:**
- **Tiny model** (`models/tiny-gpt2`) + **short run** (`max_steps=500`) + **default IMDB dataset** + potentially **zero LoRA targets** ⇒ *very* quick, but not meaningful for your memory objectives.

**Concrete issues to fix:**
1. **No adapter/model wiring.** The adapters are not attached to any transformer block. The LM does not “see” the memory features during forward.
2. **No dataset from your synthetic JSONL.** The trainer ignores `data/*.jsonl` (episodic/semantic/spatial) and therefore cannot teach the adapters the intended tasks.
3. **LoRA `target_modules` unspecified.** For GPT‑2 class models, PEFT can attach to nothing unless told (e.g., `["c_attn", "c_proj"]`, optionally MLP `["c_fc", "c_proj"]`). There is also no assertion on trainable parameter count.
4. **Replay not in the batcher.** Worker/scheduler run, but their outputs are not mixed into `SFTTrainer` batches. No “sleep/replay” cycles are executed.

**Minor concerns:**
- `learning_rate` defaults (0.001 in `configs/train/default.yaml`) are high for LoRA on larger models; for tiny toy models it may be fine but keep separate configs.
- `bf16=True` in SFTConfig may fail on some GPUs; consider autodetection or config flag.
- Background threads: ensure clean shutdown and determinism when used in real training, not only in dry‑run.

---

## 3) Evaluation plan vs. harness

**Plan (`EVAL_PLAN.md`)** — excellent structure: baselines, suites (episodic/semantic/spatial), metrics, ablations, replay cycles, seeds/sizes.

**Harness (`scripts/eval_bench.py`)** — *intentionally trivial*: it sets predictions equal to answers to validate plumbing; reports (e.g., `reports/20250819/baseline_summary.md`) show perfect EM across presets.

**What’s missing:** A real evaluation that
- loads a **trained model + attached adapters**,
- runs **retrieval/replay where appropriate**,
- computes **EM/F1/Path Success** etc. on **held‑out** data.

---

## 4) Data review (`data/*.jsonl` and generators)

**What’s good:**
- Deterministic synthetic generators in `scripts/build_datasets.py` for **episodic** (W4 with distractors, reward/pin), **semantic** (multi‑hop, contradictions, schema fit/mismatch), **spatial** (grid variants). ✅
- JSONL fields like `prompt`, `answer`, `reward`, `pin` line up with gating & evaluation needs. ✅

**What to add before serious training:**
- **Splits:** Train/val/test separation (e.g., `_train.jsonl`, `_val.jsonl`, `_test.jsonl`) with disjoint seeds.
- **Instruction formatting:** Convert to instruction‑tuning style (e.g., “system/user/assistant” or a single `text` field with “{{prompt}}\nAnswer: {{answer}}”). TRL’s SFT expects a single text field unless using a custom collator.
- **Difficulty curve:** Episodic: variable distractors, entity set growth; Semantic: hop depth 2→3→4; Spatial: map size and obstacle density sweeps.
- **Distribution shift tests:** e.g., unseen names/places/templates in test to measure generalization.
- **Noise/typos and temporal gaps** to probe robustness and the “delay” condition described in `EVAL_PLAN.md`.

---

## 5) Concrete action plan (short path to meaningful results)

### A) Wire adapters into the LM
- Implement a **patcher** that inserts adapters after block **N** of the transformer (configurable). Two common approaches:
  1. **Module replacement:** wrap `model.transformer.h[N]` (GPT‑2) or the equivalent block and add a residual `+ adapter(hidden_states, traces)` branch.
  2. **Forward hooks:** register hooks on the block outputs and apply adapters there (clean but mind autograd + checkpointing).
- Provide a `MemoryFusionConfig` with flags: which adapters to enable, where to insert, how many tokens from memory to attend over, and gating thresholds.
- Expose a clean API to **feed traces/graph features/plan embeddings** into the adapter per batch.

### B) Train on your synthetic suites
- Extend `TrainConfig` with either `dataset_files` or `suite` + `data_dir`. Support:
  ```yaml
  data:
    format: jsonl
    train: data/episodic_1000_1337.jsonl
    val:   data/episodic_200_2025.jsonl
  ```
- Add a small loader that maps JSONL items to a `text` field (e.g., `"{prompt}\nAnswer: {answer}"` for SFT) or uses TRL’s **completion‑only** collator if you want to train strictly on the answer span.
- **Mix replay into batches:** Create a `ReplayIterableDataset` that yields batches by your ratio (e.g., 50% episodic, 30% semantic, 20% fresh), reading from the scheduler + the JSONL datasets. Or implement a custom **`Trainer.get_train_dataloader()`** to concatenate iterables.

### C) Make LoRA effective
- Set `target_modules` per architecture:
  - **GPT‑2:** `["c_attn", "c_proj"]` (optionally MLP: `["c_fc", "c_proj"]`).
  - **LLaMA/Mistral:** `["q_proj", "k_proj", "v_proj", "o_proj"]` (optionally `["gate_proj", "up_proj", "down_proj"]` for MLP).
- After PEFT init, **log trainable parameter count** and **assert > 0**.
- Add configs for a small but capable base model (fits 12 GB with QLoRA): e.g., Qwen2‑1.5B‑Instruct, Phi‑3‑mini‑4k‑instruct. Keep `models/tiny-gpt2` only for CI.
- Consider **longer runs** (e.g., 3–5k steps) at lower LR (`1e‑4` → `5e‑5`) with warmup; log loss/EM on the val split.

### D) Turn the evaluation into a real benchmark
- New script `scripts/eval_model.py` that loads: base model + merged LoRA (**or** model with active adapters), your memory stores, and runs:
  - **HEI‑NW:** partial‑cue episodic recall with delay condition (pre/post replay).
  - **SGC‑RSS:** multi‑hop QA with contradictions & schema flips.
  - **SMPD:** path success, path suboptimality, macro reuse rate.
- Emit per‑suite metrics + confidence intervals over seeds; write markdown to `reports/YYYYMMDD/*.md` (keep the nice tables).

### E) Data & realism enhancements
- Add **entity/template pools** that don’t overlap between train and test.
- For episodic, introduce **time gaps** and **distractor density** scaling.
- For semantic, vary **schema templates** and add **soft contradictions**.
- For spatial, randomize **graph topology** and **stochastic success** on edges to encourage macro learning.

### F) Repro/logging/ci hygiene
- Log key config in `outputs/run-*/config.yaml` and **echo trainable params** + matched target modules.
- Ensure background workers **shut down cleanly** (you already try/finally stop them—good).
- Keep `scripts/eval_bench.py` for CI plumbing but segregate from the real evaluation.

---

## 6) Quick “how-to” for a meaningful first run

1. **Pick a real small model** (fits 12 GB with 4‑bit): e.g. `Qwen2.5-1.5B-Instruct` or `Phi-3.5-mini`. Add a `configs/model/qwen2-1.5b.yaml` with tokenizer & chat template.
2. **Wire EpisodicAdapter at block N** (e.g., after the last third of blocks).
3. **Train on episodic suite:**
   ```bash
   python scripts/train_lora.py      model_name=Qwen/Qwen2.5-1.5B-Instruct      data.format=jsonl      data.train=data/episodic_1000_1337.jsonl      data.val=data/episodic_200_2025.jsonl      lora_r=16 lora_alpha=32      target_modules='["q_proj","k_proj","v_proj","o_proj"]'      max_steps=3000 learning_rate=5e-5 gradient_accumulation_steps=8      replay.enabled=true
   ```
4. **Evaluate:**
   ```bash
   python scripts/eval_model.py suite=episodic preset=memory/hei_nw seeds='[1337,2025,4242]'
   ```

---

## 7) Minor code observations (nice-to-fix)
- `hippo_mem/episodic/store.py` imports `faiss` directly while `hippo_mem/retrieval/faiss_index.py` provides a NumPy fallback. Consider a consistent abstraction + graceful fallback for CPU‑only setups.
- Consider unifying **adapter configs** (episodic/relational/spatial) into a shared dataclass plus type‑specific extensions; many options overlap.
- Add type hints in a few places that return `np.ndarray` vs. `list[float]` for clarity.

---

## 8) Verdict

- **Spec compliance:** ~**0.6 / 1.0** — core components faithfully follow the design, but the decisive integration layer is missing.
- **Testing plan soundness:** Conceptually **strong** in `EVAL_PLAN.md`, but not yet **implemented**; current harness is CI‑only.
- **Training data suitability:** **Promising** synthetic sets; needs splits, instruction formatting, and actual use by the trainer.
- **Why training was “too fast”:** tiny model + short run + likely no LoRA targets + not using your task datasets; perfectly explains the speed and the lack of useful learning.

**Priority next steps:** (1) attach adapters into the model forward; (2) train on your JSONL suites with explicit LoRA targets; (3) implement the real evaluation script. After these, you’ll have credible, slower runs and meaningful curves/metrics.

---

*End of review.*
