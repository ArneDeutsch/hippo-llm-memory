# hippo-llm-memory — Post-Review Gap-Closure Analysis
**Date:** 2025-08-20  
**Reviewer:** GPT-5 Thinking  
**Source artifact (zip) SHA-256:** `d3d832a1503511c1692af4b4358a84bfd1c49551fc0bbf5a3d905d1aa4020bdd`

> Scope: Validate whether the issues raised in *review/review-2025-08-19.md* are now addressed; determine if **Milestone 7b** is complete and whether the repo is ready to proceed to **Milestone 8a**. Evidence cites concrete files/lines where possible.

---

## 0) Executive summary

- **Major 7b gaps from 2025‑08‑19 are closed.** There is now a concrete **adapter patcher** that injects Episodic/Relational/Spatial adapters into the Transformer **forward path** (configurable block index), LoRA **target modules** are inferred and validated, a **JSONL loader** is used instead of IMDB, and **replay mixing** is integrated into the training dataloader with tests.
- **Confidence: high** for 7b scope. Unit tests exist for wiring and trainables; a dedicated **smoke script** exists but has two minor string mismatches (see §2.5) that you should fix before relying on it as a gate.
- **Evaluation harness is still a lightweight CI stub** (as expected pre‑8a); real retrieval & write‑gate plumbing for adapters remains the next milestone.
- **Verdict:** **Milestone 7b — PASS (with nits).** You can proceed to **8a Runtime retrieval & write‑gate plumbing**.

---

## 1) Alignment against the 2025‑08‑19 review (then → now)

### 1.1 Adapter wiring into model forward
- **Then (gap):** Adapters existed but were not inserted into the LM’s forward; no hooks or module replacement.  
- **Now (fixed):** `hippo_mem/adapters/patch.py` implements `attach_adapters(...)` and an `AdapterFusion` wrapper that **wraps a specific transformer block’s `forward`** and adds residual branches for episodic/relational/spatial adapters. Tuple outputs are handled.  
  **Evidence:** `hippo_mem/adapters/patch.py` (`AdapterFusion`, `find_transformer_blocks`, `attach_adapters`); usage in `scripts/train_lora.py` (logging “Adapter fusion attached …”).  
  **Tests:** `tests/test_adapter_wiring.py` asserts the adapter is invoked exactly once per forward.

### 1.2 Trainer consumes JSONL suites (no IMDB)
- **Then (gap):** Trainer hard‑coded to HF `imdb`.  
- **Now (fixed):** `scripts/jsonl_dataset.py` loads JSONL files with `prompt/answer` → **`text`** field (“instruction formatting” as suggested). `TrainConfig` exposes `data_format=jsonl`, `train_files`, `val_files`; error if missing outside dry‑run.  
  **Evidence:** `scripts/train_lora.py` (branch on `cfg.data_format == "jsonl"`); `configs/train/default.yaml` (defaults to JSONL).  
  **Tests:** `tests/test_data_loader.py` verifies `text` field; datasets present under `data/` with **checksums**.

### 1.3 Replay mixing in dataloader
- **Then (gap):** Scheduler existed but not interleaved into training batches.  
- **Now (fixed):** `scripts/replay_dataset.py` defines `ReplayIterableDataset` that **probabilistically injects replay items** into the base stream at a configurable ratio. `train_lora.py` wraps the base dataset when `replay.enabled`.  
  **Evidence:** `scripts/train_lora.py` (construction of `ReplayIterableDataset` and log “Replay mixing active …”).  
  **Tests:** `tests/test_replay_dataset.py` verifies interleaving; `tests/test_replay_scheduler.py` checks queue ranking and episodic/semantic mix.

### 1.4 LoRA target modules & trainables assertion
- **Then (gap):** No architecture‑specific `target_modules`; no assertion on trainables > 0.  
- **Now (fixed):** `hippo_mem/adapters/lora.py` provides `default_target_modules(model)` with fallbacks (GPT‑2, LLaMA/Mistral, inspection of first block). `train_lora.py` uses either explicit `cfg.target_modules` or inferred defaults, and in **dry‑run** attaches LoRA and **asserts** `count_trainable_parameters(model) > 0`.  
  **Evidence:** `hippo_mem/adapters/lora.py`; `scripts/train_lora.py` (logs “Target modules: …”, counts trainables, raises if 0).  
  **Tests:** `tests/test_lora_targets.py` confirms defaults yield non‑zero trainables.

### 1.5 Minimal end‑to‑end training path
- **Then (gap):** Short runs “too fast” and meaningless for memory objectives.  
- **Now (partial by design):** The loop is still intentionally **minimal** (TRL SFT on JSONL) but now *exercises* adapter wiring and replay mixing. This is sufficient for **7b**; meaningful curves will come with **8a** when real memory tokens are fed to adapters.

---

## 2) Milestone 7b Gate — checklist & evidence

> **Gate expectation:** “dry‑run green; unit tests for wiring & trainables pass; short local run logs show adapter activation, non‑zero trainables, and JSONL consumption.”

### 2.1 Adapter hookup — ✅
- **Files:** `hippo_mem/adapters/patch.py`, `scripts/train_lora.py`
- **Test:** `tests/test_adapter_wiring.py`

### 2.2 LoRA attachment checks — ✅
- **Files:** `hippo_mem/adapters/lora.py`, `scripts/train_lora.py`
- **Test:** `tests/test_lora_targets.py` and dry‑run branch in `train_lora.py`

### 2.3 JSONL data loader — ✅
- **Files:** `scripts/jsonl_dataset.py`, `configs/train/default.yaml`, `data/*.jsonl`, `data/checksums.txt`
- **Test:** `tests/test_data_loader.py`

### 2.4 Replay mixing — ✅
- **Files:** `scripts/replay_dataset.py`, `scripts/train_lora.py`
- **Tests:** `tests/test_replay_dataset.py`, `tests/test_replay_scheduler.py`

### 2.5 Smoke & CI gates — ⚠️ *nits*
- `scripts/smoke_7b.sh` greps for substrings that **don’t match** current logs:
  - Grep: `trainable params` vs log: **“Trainable parameters”** (spelling mismatch).  
  - Grep: `Loaded JSONL` — there is no such log; you log **“Train dataset size: …”**.
- **Fix suggestion:** change greps to
  ```bash
  grep -iq "Trainable parameters" "$LOG"
  grep -iq "Train dataset size" "$LOG"
  ```
  and ensure `set -euo pipefail` plus meaningful exit codes.

### 2.6 Thread lifecycle — ⚠️ *minor*
- The consolidation worker is cleanly stopped in a `finally` block. Background maintenance in `EpisodicStore` / `KnowledgeGraph` / `PlaceGraph` runs via **daemon** threads; there is no explicit stop. Acceptable for experiments, but if you begin longer training runs in 8/9, consider optional `stop_background_tasks()` for determinism in tests.

**Gate decision:** **PASS with nits** (fix §2.5 soon).

---

## 3) Will this implementation *verify the algorithms* from `research/experiment-synthesis.md`?

- **HEI‑NW (episodic):** The key components exist and are now wired: write gate (`hippo_mem/episodic/gating.py`), FAISS+SQLite store with Hopfield‑style completion (`hippo_mem/episodic/store.py`), prioritized replay (`hippo_mem/episodic/replay.py`), and cross‑attention adapter (`hippo_mem/episodic/adapter.py`). **What’s missing for true verification** is **runtime retrieval + write path integration** so that *actual recalled traces* become adapter inputs and *new episodes* are gated and persisted during ...
- **SGC‑RSS (relational/semantic):** Tuple extraction, schema index, KG store, and adapter are implemented (`hippo_mem/relational/*`). As with episodic, **8a retrieval packing** is required to feed graph neighborhoods (or embeddings) into the adapter during forward and to route low‑confidence tuples back to episodic for replay.
- **SMPD (spatial):** Place graph and adapter are implemented (`hippo_mem/spatial/*`). For verification you’ll need **projection/packing** of local subgraph/plan tokens for the adapter (8a) and, later, policy‑style replay (Milestone 9).

**Conclusion:** The 7b code positions you to **empirically** verify the designs after **8a** adds the real retrieval/packing API. The present state suffices to test the *plumbing* (wiring, data, replay), not the *capabilities* yet.

---

## 4) Additional observations vs the review

- **Instruction formatting & splits:** Addressed. JSONL loader generates `text = "{{prompt}}
Answer: {{answer}}"` and `split_train_val` provides a deterministic 95/5 split when no val file is given. Synthetic suites in `data/` cover sizes (50/200/1000), variants (episodic distractors, hop depth, grid sizes/densities) and multiple **seeds** with **checksums**.
- **Adapter config hygiene:** Nice consolidation via `MemoryFusionConfig` and per‑module adapter configs; ablation flags in tests (`tests/test_training.py::test_adapter_ablation_flags`). 
- **Defaults:** `learning_rate=1e-3` in `configs/train/default.yaml` is aggressive for 4‑bit small models with adapters; you already have `configs/train/qlora.yaml` at `1e-4`. Consider model‑specific overrides.
- **PROJECT_PLAN.md status:** 7b checkboxes remain unchecked; update them to reflect the current state (see §2).

---

## 5) Recommendations before starting 8a

1. **Fix the smoke test greps** and add it to CI so 7b stays green.  
2. **Update `PROJECT_PLAN.md`**: mark 7b items done; add a short “evidence” bullet per item with file paths (can copy from §2).  
3. **Sketch 8a adapter I/O:** Introduce a `TraceSpec`/`MemoryTokens` dataclass and thread it through `attach_adapters` so adapters accept `memory_tokens` + masks; keep proxies as a fallback when empty.  
4. **Retrieval hooks:** Prototype E2E for episodic first: batch → cue encoding → `store.recall/complete` → projection to `d_model` → pack `[B, M, d_model]` → adapter. Log K, latency, and hit‑rates.  
5. **Optional:** Add `stop_background_tasks()` for stores/maps to aid deterministic teardown in tests.

---

## 6) Verdict

- **Are the gaps closed?** Yes, all major 7b gaps called out by the 2025‑08‑19 review are **closed**; only minor hygiene issues remain (smoke grep strings, optional thread stop).  
- **Is the produced code complete for 7b?** Yes.  
- **Have the suggested improvements been implemented?** Yes for 7b scope (wiring, LoRA, JSONL, replay, tests). The evaluation‑as‑science aspects remain intentionally for **8a/9**.  
- **Will this implementation serve the goal to verify the algorithms?** Yes — after **8a** adds runtime retrieval/packing & write‑gate plumbing; the current state is a solid foundation.  
- **Is 7b ready and can we continue with 8a?** **Yes. Proceed to Milestone 8a.**

---

*Appendix — Pointers*
- Adapter fusion/wiring: `hippo_mem/adapters/patch.py` · Tests: `tests/test_adapter_wiring.py`
- LoRA helpers: `hippo_mem/adapters/lora.py` · Tests: `tests/test_lora_targets.py`
- JSONL loader: `scripts/jsonl_dataset.py` · Tests: `tests/test_data_loader.py`
- Replay: `scripts/replay_dataset.py`, `hippo_mem/episodic/replay.py` · Tests: `tests/test_replay_dataset.py`, `tests/test_replay_scheduler.py`
- Training entrypoint: `scripts/train_lora.py`  
- Smoke script: `scripts/smoke_7b.sh` (fix greps per §2.5)
- Data suites + checksums: `data/`

*End of analysis.*
