# Default evaluation configuration.
tasks: []

# Basic run parameters with reasonable defaults.
suite: null
preset: baselines/core
n: 5
seed: 0
date: null
outdir: null
dry_run: false
model: models/tiny-gpt2
max_new_tokens: null
use_chat_template: null
system_prompt: null
pad_token_id: null
eos_token_id: null
force_chat: null
force_no_chat: null

# Report macro averaged scores in addition to micro scores.
macro_scoring: true

# Number of replay cycles to run between evaluations in ``eval_bench``.
post_replay_cycles: 0

# Replay evaluation options for ``eval_model`` (kept for backwards compat).
replay:
  cycles: 0

# Hydra stays in-place so our manual ``outdir`` handling works.
hydra:
  run:
    dir: .
  output_subdir: null

# Placeholder for ablation flags exposed via ``+ablate=...`` on the CLI.
ablate: {}
