# Default evaluation configuration.
tasks: []
suites: []
presets: []
n_values: [50, 200, 1000]
seeds: [1337, 2025, 4242]

# Basic run parameters with reasonable defaults.
# ``task`` is kept for CLI convenience and maps to ``suite`` internally.
task: null
suite: ${task}
preset: baselines/core
n: 5
seed: 0
date: ${oc.env:RUN_ID,${oc.env:DATE,null}}
outdir: null
dry_run: false
model: models/tiny-gpt2
max_new_tokens: null
use_chat_template: null
system_prompt: null
pad_token_id: null
eos_token_id: null
force_chat: null
force_no_chat: null
mode: test
store_dir: null
session_id: null
persist: false
memory_off: false
gating_enabled: false
primary_em: norm

# Report macro averaged scores in addition to micro scores.
macro_scoring: true

# Number of replay cycles to run between evaluations in ``eval_bench``.
post_replay_cycles: 0

# Replay evaluation options for ``eval_model`` (kept for backwards compat).
replay:
  cycles: 0

# Shortcut overrides folded into ``memory.*`` by the harness.
episodic:
  gate:
    tau: 0.5
relational:
  gate:
    threshold: 0.6
spatial:
  gate:
    block_threshold: 1.0

# Hydra stays in-place so our manual ``outdir`` handling works.
hydra:
  run:
    dir: .
  output_subdir: null

# Placeholder for ablation flags exposed via ``+ablate=...`` on the CLI.
ablate: {}
