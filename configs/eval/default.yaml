# Default evaluation configuration.
tasks: []
suites: []
presets: []
n_values: [50, 200, 1000]
seeds: [1337, 2025, 4242]

# Basic run parameters with reasonable defaults.
# ``task`` is kept for CLI convenience and maps to ``suite`` internally.
task: null
suite: ${task}
preset: baselines/core
n: 5
seed: 0
run_id: ${oc.env:RUN_ID,null}
outdir: null
dry_run: false
model: ???
dataset_profile: default
max_new_tokens: null
replay_cycles: 0
use_chat_template: null
system_prompt: null
pad_token_id: null
eos_token_id: null
force_chat: null
force_no_chat: null
mode: test
store_dir: null
session_id: null
persist: false
memory_off: false
gating_enabled: true
strict_telemetry: false
allow_dummy_stores: false
primary_em: norm
no_retrieval_during_teach: true
isolate: none

# Control evaluation-time compute features.
compute:
  # Whether to calculate per-item metrics during the pre-replay sweep.
  # Disable via ``--no-pre-metrics`` for quicker smoke tests.
  pre_metrics: true
  # Enable oracle readers for upper-bound metrics.
  oracle: false

# Report macro averaged scores in addition to micro scores.
macro_scoring: true

# Number of replay cycles to run between evaluations in ``eval_bench``.
post_replay_cycles: 0

# Replay evaluation options for ``eval_model`` (kept for backwards compat).
replay:
  cycles: 0
  samples: 3

# Shortcut overrides folded into ``memory.*`` by the harness.
# ``memory`` root key intentionally omitted so baselines don't load memories.
episodic:
  gate:
    tau: 0.5
relational:
  gate:
    threshold: 0.6
spatial:
  gate:
    block_threshold: 1.0

# Hydra stays in-place so our manual ``outdir`` handling works.
hydra:
  run:
    dir: .
  output_subdir: null

# Placeholder for ablation flags exposed via ``+ablate=...`` on the CLI.
ablate: {}
