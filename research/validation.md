# Hippocampus-Inspired Memory Systems for LLMs: Novelty and Related Work

The three proposed memory designs draw heavy inspiration from the hippocampal memory system – covering **episodic**, **semantic**, **spatial**, and **procedural** memory facets. We analyze each design’s novelty by comparing with existing research (including recent preprints), and summarize similar approaches and their results. We also highlight connections to *continual learning* (mitigating catastrophic forgetting via memory replay), which is a key motivation behind such architectures.

## 8.1 **Hippocampal Episodic Index with Neuromodulated Writes (HEI-NW)**

**Idea:** This design augments an LLM with a persistent **episodic memory** that stores new experiences in a one-shot manner. It uses **pattern separation** (a sparse key encoding like the dentate gyrus) and **associative recall** (a Hopfield-like auto-associative network akin to CA3) to enable *partial cue* retrieval of entire past episodes. Importantly, **neuromodulator-inspired gating** decides when to write an episode to memory based on novelty, surprise, or reward (analogous to dopamine/norepinephrine signals), and high-salience episodes are later **replayed offline** to consolidate their knowledge into the model’s weights (as per Complementary Learning Systems theory). Low-salience memories decay over time to manage capacity.

**Novelty vs. Existing Work:** The concept of an LLM with an *external episodic memory* and controlled consolidation is **actively being explored** in recent research. The overall philosophy follows the well-known Complementary Learning Systems (CLS) from neuroscience – a fast hippocampal system for one-shot episodic learning and a slow neocortical system for gradual statistical learning. Indeed, *Larimar* (Das et al., 2024) proposes a very similar architecture: a “brain-inspired” LLM with a **distributed episodic memory** that allows dynamic one-shot knowledge updates without retraining. Larimar explicitly frames the base LLM as the “neocortex” and the memory as a hippocampus analog, enabling test-time adaptation to new facts. It demonstrates the viability of such one-shot memory: after injecting new facts into memory, the LLM’s outputs change immediately, achieving accurate editing of knowledge with **no gradient updates** and no forgetting of unedited facts. Notably, Larimar attains comparable accuracy to SOTA fine-tuning-based editors on benchmarks, but *4–10× faster*, since memory writing is fast. It also supports **selective forgetting** by removing entries from memory, something parametric editors struggle with. This shows that one-shot *episodic writes* with proper gating can offer both **speed and reversibility** in updating an LLM’s knowledge.

Another very relevant approach is *EM-LLM* (Fountas et al., 2025), which is *explicitly inspired by human episodic memory*. EM-LLM segments a continuous text stream into **episodic events** using a *Bayesian surprise* metric to detect boundaries – analogous to neuromodulatory novelty signals – and stores these events. When needed, it retrieves past events with a **two-stage process**: first semantic similarity search, then a *temporal linking* step that brings in contiguous events (simulating how hippocampal recall often triggers sequences). This human-like episodic memory system enabled *practically infinite context* for LLMs: EM-LLM handled up to **10 million tokens** of context via memory retrieval, far beyond the model’s normal window. Impressively, it outperformed both a standard retrieval-augmented model (RAG) and even models given the *full* long context in tasks like QA and summarization. In short, episodic memory + surprise-based event indexing gave better long-range coherence than brute-force long attention, confirming the strength of hippocampal strategies.

**Pattern separation/completion:** The use of **sparse keys and associative recall** in HEI-NW also has precedents. For instance, *HippoMM* (Lin et al., 2025) is a multimodal memory system that explicitly implements *dentate gyrus–like pattern separation* and *CA3-like pattern completion* for continuous video streams. HippoMM dynamically segments an input stream into discrete episodes, encodes them with a **sparse high-dimensional code**, and can robustly complete an entire event when given a partial cue (even cross-modal, e.g. recalling a video scene from an audio clip). This led to large gains in long video understanding; HippoMM outperformed state-of-art retrieval systems (78.2% vs 64.2% accuracy) while also **5× faster** in query response. These results echo the benefits of pattern-separated episodic indexing plus Hopfield-style retrieval for accurate memory recall. Although HippoMM was for multimodal data, the same principles apply to LLM text memories in HEI-NW.

**Neuromodulated gating & replay:** The idea of gating memory writes by novelty/salience has grounding in **neuroscience** and some AI works. EM-LLM’s use of “Bayesian surprise” to decide event boundaries is one example of novelty gating in practice. More generally, *Generative Replay* strategies in continual learning play a similar role: e.g. **Hippocampal Memory Indexing (HMI)** (Yin et al., 2023) stores compressed representations of training samples (using product quantization for sparse encoding) and re-generates them for replay. HMI was used to **continually train** language models on sequences of tasks without forgetting. By replaying indexed past samples, it significantly outperformed a baseline LAMOL system on both natural language understanding tasks and domain adaptation, and was more robust to task order variations. This aligns with HEI-NW’s proposal: important experiences (high surprise or reward) are stored and replayed later to consolidate knowledge, mitigating *catastrophic forgetting*. In fact, episodic memory with replay is a known remedy in *continual learning* – it prevents the model from overwriting old knowledge by interleaving old samples during new training. Thus, HEI-NW’s replay mechanism is well-supported: experiments across NLP tasks show that models with episodic replay suffer less forgetting and achieve higher final performance than those without.

**Summary:** Overall, the HEI-NW design is **not entirely novel** – it is strongly supported by recent research trends combining LLMs with **fast episodic memory modules**. Key elements like one-shot memory writing, associative retrieval, and periodic replay have each been explored (Larimar, EM-LLM, HMI, etc.) with **successful results**. The novelty of HEI-NW may lie in the particular combination (e.g. using a modern Hopfield network for recall, or biologically faithful neuromodulatory thresholds), but similar principles have yielded clear benefits: *faster adaptation*, *handling of “infinite” contexts*, *reduced forgetting* in lifelong learning, and *memory-efficient knowledge updates* without gradient training. All these outcomes align with what HEI-NW seeks to achieve. We therefore consider HEI-NW an evolution of a **burgeoning area** of memory-augmented LLM research, rather than a completely new idea. The approach is strongly justified by both computational results and neuroscience insights (e.g. CLS theory).

## 8.2 **Schema-Guided Consolidation with Relational Semantic Store (SGC-RSS)**

**Idea:** This design introduces a dedicated **semantic memory store** in the form of a relational knowledge graph. When a new episode is encountered, it is parsed into structured elements (entities, relations, contexts, etc.). A *schema-matching* mechanism then assesses if the episode fits into existing schemas/knowledge: if it is **schema-congruent** (follows known patterns), it is fast-tracked for **immediate integration** into the semantic store (and only light replay is needed). If it is **schema-incongruent** (novel structure), it remains in episodic memory and undergoes extensive replay until gradually absorbed into the semantic store. During inference, queries draw from both semantic memory (for general facts/graphs) and the episodic store (for specific instances or outliers). A scheduler (inspired by CA2) interleaves replay events to prevent interference, ensuring new information doesn’t erase older knowledge (addressing catastrophic forgetting). This design mirrors how having prior schemas can accelerate learning and consolidation in the brain.

**Novelty vs. Existing Work:** The idea of coupling **episodic memory with a structured knowledge base** is increasingly seen in LLM-based agent architectures. A notable example is *AriGraph* (Anokhin et al., 2025), a system where an autonomous LLM agent *builds and updates a knowledge graph* of its environment as it explores. AriGraph’s **memory graph** explicitly integrates **episodic observations** with **semantic world knowledge**: as the agent interacts (e.g. in a text-based adventure game), it adds nodes/edges for new entities and relations (facts), while also logging specific events. This structured memory greatly improved reasoning and planning. The authors report that an LLM agent with AriGraph memory **outperforms** other memory methods (like vector databases or unstructured summaries) and even strong RL baselines on complex interactive text tasks. In fact, AriGraph’s agent could solve difficult text-game quests that humans struggle with, thanks to storing a *graph of connected knowledge* to reason over. It also achieved competitive results on static knowledge tasks: e.g. multi-hop QA performance approached that of dedicated knowledge-graph-based QA systems. This demonstrates the power of a **relational semantic store** for an LLM’s long-term memory – very much in line with SGC-RSS. The notion of *aligning new experiences to existing schema* is implicit in AriGraph: since the agent incrementally merges observations into the graph, new facts that connect to existing nodes are naturally integrated faster, whereas completely new structures just create new graph components. SGC-RSS makes this process explicit by scoring schema similarity, but the end effect is similar: use prior knowledge to organize and consolidate memories.

Another closely related concept is found in *Generative Agents* (Park et al., 2023), an influential simulation of long-lived AI agents. Generative Agents record *every event* an agent experiences into an **episodic memory stream**. They then perform periodic **“reflection”**: the agent reviews its raw memories to infer higher-level **facts and beliefs** (e.g. inferring “Alice is friends with Bob” after several interactions). These inferred facts are stored back into memory as summaries, effectively building a structured semantic understanding from episodic data. This is analogous to forming schemas. The agent uses a retrieval mechanism that favors *recent* and *important* memories, and can generalize via those reflections. The result was striking – the generative agents exhibited **believable, consistent behavior over long time spans** (multiple days of simulated in-game life), something a vanilla LLM cannot do. They remembered past interactions, followed social norms, and adapted to new events by updating their “belief” store. While not explicitly a graph database, the combination of episodic logs + extracted facts in Generative Agents serves a similar purpose to SGC-RSS’s dual memory: specific episodes for detailed context, and a **semantic store of distilled knowledge** for general reasoning. This again shows that maintaining *structured, consolidated knowledge* alongside raw episodes is highly beneficial for long-term consistency.

**Schema-accelerated learning:** The idea that *schema congruence leads to faster consolidation* has empirical support in neuroscience. When new information fits an existing schema, the hippocampus can train the neocortex much more rapidly. Human studies have found that schema-consistent events are remembered better after a delay (especially the gist), and show increased post-learning coupling between hippocampus and prefrontal cortex – presumably reflecting quick integration into cortical networks. In contrast, novel, schema-incongruent events rely longer on hippocampal storage and require more replay (e.g. multiple nights of sleep) to consolidate. SGC-RSS directly leverages this principle. We did not find an existing LLM memory module that *explicitly* implements a “schema score” gating consolidation as described, so this particular mechanism may be **novel in implementation**. However, elements of it are apparent in prior work. For example, *Chenet al.* (2023) used a **knowledge graph memory** for a medical chatbot and noted that new facts which overlapped with the existing graph could be added with minimal retraining. And the *Structural Knowledge in AI* survey (Wu et al., 2023) argues that cognitive maps and schemas should be used to organize an AI’s knowledge, citing that graph-structured memory can improve generalization. These align with SGC-RSS’s motivations.

**Avoiding interference:** The use of a **replay scheduler** to interleave new and old knowledge is a known strategy in continual learning. It’s inspired by hippocampal *sharp-wave ripple* events where recent and remote memories are reactivated in an interwoven manner. In AI, *experience replay* buffers often mix recent and past samples to prevent catastrophic forgetting. *GRACE* (Hartvigsen et al., 2022) and other LLM editing methods attempted to address interference by editing in latent space or using multiple memory slots, but they were complex and still forgot earlier edits eventually. In contrast, approaches like Larimar’s memory (as in 8.1) or AriGraph’s structured memory naturally retain old info unless explicitly removed. SGC-RSS would follow suit, using its CA2-like planner to ensure no two similar memories are replayed back-to-back (reducing overlap in weight updates). This specific detail has not been widely reported in literature, but it’s a reasonable extension of known techniques.

**Summary:** The concept of **integrating episodic and semantic memory** for LLMs is gaining traction, making SGC-RSS timely. The use of an explicit knowledge graph (or database of facts/tuples) alongside an episodic buffer has clear precedents (AriGraph’s memory graph, and to an extent Generative Agents’ distilled beliefs). These systems have shown improved performance on complex reasoning and long-horizon tasks by leveraging structured knowledge. What’s relatively novel is the *schema alignment heuristic* – prioritizing schema-fitting information for immediate consolidation. This is strongly rooted in neuroscience and intuitively appealing, though we have not seen it articulated in existing LLM memory implementations. If realized, SGC-RSS could combine the strengths of knowledge graphs (precise, queryable memory) with episodic recall (detailed context), yielding an LLM that learns **continuously** and robustly. Given existing evidence, one can expect benefits like better factual consistency, faster learning of routine patterns, and less forgetting of long-tail exceptions (since those remain in episodic store until sufficiently replayed). In summary, SGC-RSS builds on known ideas of graph-based memory in AI, adding a biologically-inspired twist that appears both **novel and promising**.

## 8.3 **Spatial Map + Replay-to-Policy Distillation (SMPD)**

**Idea:** This design equips the LLM (or LLM-based agent) with a dedicated **spatial memory** module and a mechanism to learn **procedural skills** from repeated experiences. The spatial module constructs a kind of **cognitive map**: nodes represent “places” or distinct contexts (which could be physical locations or abstract states), and edges represent transitions or relationships (with possible weights for distances, costs, etc.). As the agent experiences a sequence (in a text adventure, a tool-using sequence, or a real environment), it performs a form of **path integration** to update its position in this latent map or create new nodes. Given a goal, the agent can plan by doing a graph search over this map to find a sequence of steps (a route) to the goal, which the LLM can then execute or elaborate in detail. In parallel, the design proposes a **procedural memory**: by replaying past successful trajectories, a small policy model or “skill head” is trained (via behavioral cloning or distillation) to turn frequently used action sequences into **macro-actions** (reusable routines). These learned skills serve as a library of habits or subroutines that the LLM can invoke for efficiency. The spatial nodes can also link to the episodic and semantic memory (from designs 8.1 and 8.2), enabling location-based recall (e.g. “remember what happened last time I was here”) or using factual knowledge about a location during planning.

**Novelty vs. Existing Work:** The **spatial map** component is directly inspired by the role of the hippocampus in navigation (place cells, cognitive maps), and similar ideas have appeared in deep learning, especially in reinforcement learning contexts. A classic example is the *Neural Map* architecture (Parisotto & Salakhutdinov, 2018), which introduced a **structured 2D memory grid** for an RL agent to record visited locations and observations. Neural Map provided the agent with an explicit spatial layout memory (writeable and readable by coordinates). It *surpassed prior memory architectures* on maze navigation tasks in both 2D and 3D environments, and notably could **generalize to new maps** despite never seeing them in training. This demonstrates that having a map-like memory greatly aids in navigation and memory of locations. Similarly, *Neural SLAM* approaches and differentiable planners have shown that learning an internal topological map leads to more efficient exploration and goal-finding. The SMPD spatial module is very much in line with these successes. While those were used in smaller RL models, the concept is now entering the LLM agent domain. For instance, *AriGraph* (mentioned earlier) essentially gave an LLM agent a world **graph** where locations, objects, and agents were nodes. Although framed as a knowledge graph, it served as a **world model** that the agent could plan over. The Ariadne LLM agent using AriGraph memory was able to plan multi-step solutions in text games far better than an LLM with no structured memory – effectively the agent learned to navigate its state-space using the graph as a map. Thus, the idea of a spatial/graph memory for an LLM-based agent is already yielding results, validating this part of SMPD.

**Procedural skill distillation:** The second aspect – replaying trajectories to train **macro-policies** – connects to work in both reinforcement learning and emerging LLM agent research. In reinforcement learning, the concept of **options** or **macro-actions** is well-studied: agents learn reusable skills to speed up long-horizon tasks. Here, instead of learning options purely via gradient RL, SMPD suggests using the LLM’s own experiences (successful task solutions) to *imitation-train* a policy head. This resembles the approach of *Voyager* (Wang et al., 2023), which is “the first LLM-powered lifelong learning agent in Minecraft”. Voyager uses GPT-4 to autonomously explore Minecraft, and as it figures out how to achieve things (like crafting items or fighting monsters), it saves those **successful action sequences as Python functions** in an ever-growing **skill library**. Over time, Voyager accumulates a large repertoire of skills (tools) which it can call later instead of recomputing from scratch. The effect was dramatic: Voyager became *increasingly competent* and achieved game milestones much faster than baseline agents. For example, it obtained **3.3× more unique items**, traveled **2.3× longer distances**, and reached key tech milestones up to **15× faster** than the previous state-of-the-art agent, thanks to its learned skill library. Moreover, it could **generalize** its skills to a new world, solving new tasks “from scratch” by composing its learned routines, which other approaches struggled with. This is concrete evidence that **distilled procedural knowledge** (in Voyager’s case, code for skills; in SMPD’s case, a policy head’s macro-actions) greatly improves an agent’s long-term learning and adaptability. SMPD’s replay-to-policy idea follows the same vein – by imitating its own successful past (possibly accelerated via off-policy replay), an LLM agent can build a *habit memory* that makes future decision-making more efficient.

We also see analogies in robotics and planning: approaches where an AI learns high-level plans from demonstration or self-play and then reuses them. For instance, *Hierarchical GPT* experiments (2023) had a high-level GPT select sub-tasks and a low-level GPT execute them, effectively learning a form of procedural decomposition. While not explicitly described as replay distillation, it’s related in spirit to having an LLM break problems into learned subroutines. The **Generative Agents** project, too, hinted at procedural knowledge – agents would form *plans* (e.g. daily schedules) and execute them, and could revise their plans based on outcomes, which is an early form of planning and skill learning.

**Spatial memory + planning:** Combining the spatial map with planning and memory recall can yield novel capabilities. The SMPD design enables things like “recall by place” – which is actually seen in animals (context-dependent memory retrieval). In AI, we have seen *location-based context* improvements. A simple example: in text adventure games, if the agent keeps a map of rooms it has seen, it can reason “if I need an object that was in the kitchen, go back to the kitchen node” rather than scanning the entire text history. This targeted recall could make LLM agents much more efficient with long contexts. We did not find a prior paper that explicitly links a learned map to episodic memory retrieval, so that integration might be new. However, it aligns with work on **cognitive maps for knowledge**: a recent paper by **Spens et al. (2024)** trains a model to encode sequences into a latent cognitive map and replay them to train a predictor (like an LLM). They argue this explains human planning and prediction. In essence, they simulate a hippocampal map that feeds a “generative network” (akin to a world-model or language model) through replay. This provides theoretical backing that building an internal graph of experiences and using it to drive model updates (like policy learning) can boost planning capabilities – precisely what SMPD aspires to do.

**Summary:** The SMPD design is an ambitious integration of **spatial reasoning and procedural learning** into LLMs. Each piece has encouraging precedent: **graph-structured spatial memories** have improved navigation and planning in RL agents (e.g. Neural Map), and **skill distillation from past experience** has been shown to drastically improve an LLM agent’s lifelong learning (Voyager’s skill library). The novelty of SMPD may lie in tying these together with an LLM’s capabilities – for example, using the LLM to interpret a graph-derived plan and fill in details, or to leverage both the spatial map and semantic memory for contextual decision making. This cross-talk between memory systems is very true to the brain (hippocampus interacts with both cortical maps and subcortical habit systems) but relatively unexplored in AI, where memory modules are usually used in isolation. We anticipate that an LLM with SMPD-style memory could exhibit **far better problem-solving in tasks involving navigation, search, or tool use**. It would remember “where” important information or resources are (spatial memory) and accumulate “how-to” knowledge for multi-step procedures (procedural memory), all while integrating with episodic and semantic recall. In sum, SMPD builds upon known components that are individually successful and combines them in a biologically-inspired way. The approach is **partly novel** in its specific integration, but its core ideas are validated by prior research: maps help agents not get lost, and learning from past solutions yields compounding returns. This design is well-aligned with the direction of making AI agents more *embodied and memory-rich*, enabling **continual learning** of not just facts (semantic memory) but skills and environment layouts over time.

**References and Notes:**

* The above comparisons include several **preprints** (Larimar, EM-LLM, HippoMM, AriGraph, etc.) and recent conference papers, highlighting the *state-of-the-art* in memory-augmented LLM research as of 2024–2025. All source citations are provided in the format 【source†lines】.
* Each of these hippocampal-inspired designs aligns with trends in **continual learning**: using episodic memory and replay to avoid catastrophic forgetting, and using structured knowledge to build ever-growing capabilities. By combining fast memory writes with careful consolidation, they allow an LLM to *learn new information continuously* without destroying old knowledge – a key requirement for deploying LLMs in the real world, where they must adapt over time.
* Neuroscience theories (e.g. Complementary Learning Systems, schema-based consolidation, hippocampal replay for planning) are not just analogies but are increasingly being *operationalized* in AI systems. This convergence suggests that the proposed designs, though complex, stand on a firm foundation of both current AI research and biological plausibility. Each has at least partial precedents showing feasibility and benefits, making them exciting directions to pursue rather than completely uncharted ideas.
