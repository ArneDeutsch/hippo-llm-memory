# How Large Language Models Work: Architecture and Mechanisms

## Introduction

Large Language Models (LLMs) are extremely large neural networks that generate and understand human-like text. These models, developed by organizations like OpenAI, xAI, Google, and others, are typically based on the **Transformer architecture**, which enables handling long-range dependencies in text via an attention mechanism. An LLM like GPT-4 or Google’s PaLM contains *billions* of parameters and was trained on massive text corpora to predict the next word in a sequence. Through this training, the model learns statistical patterns of language, acquiring knowledge about syntax, semantics, and facts in its weights. Crucially, LLMs learn in an *unsupervised* manner (self-supervised), using the raw text itself as training data – no explicit labels are needed beyond the text context. After training, these models can be adapted to many tasks via prompts or fine-tuning, demonstrating abilities in everything from coding to question-answering, all by virtue of their learned language understanding.

**Basics of Learning:** LLMs are fundamentally neural networks trained by **backpropagation**. During training, the model reads sequences of tokens and tries to predict the next token at each step. The difference between the model’s predicted probability distribution and the actual next token (the ground truth) is measured by a loss function, usually *cross-entropy loss*. The model’s millions (or billions) of weights are then adjusted via gradient descent to minimize this loss. This process – forward pass to get predictions, loss computation, and backward pass (backpropagation) to update weights – repeats over billions of tokens. Over time, the model “learns” to produce more accurate next-token predictions. In essence, the training objective is *next token prediction*, and the result is a model that can generate coherent text by sequentially predicting one token at a time. Modern LLMs like GPT-3 were trained on hundreds of billions of words using huge compute clusters, but the core learning algorithm remains standard backpropagation and optimization (often with advanced optimizers like Adam). It’s important to note that while *training* an LLM is extremely computationally intensive, *using* the trained model (inference) is also heavy but feasible with optimized implementations on GPUs/TPUs.

## Transformer Architecture Overview

The breakthrough enabling modern LLMs is the **Transformer architecture** (introduced by Vaswani et al., 2017). Unlike earlier recurrent neural networks (RNNs) that processed text sequentially, transformers process sequences in parallel and capture dependencies via an attention mechanism. At a high level, a transformer is a deep stack of layers, each layer containing two main sub-components: a **self-attention mechanism** and a **position-wise feed-forward network**, with residual connections and normalization around them. LLMs usually use the *decoder* portion of the Transformer (for autoregressive generation), potentially stacked to dozens or hundreds of layers. Below is an illustration of the Transformer’s structure:

&#x20;*Transformer architecture diagram. Each layer (block) in the encoder (left) or decoder (right) contains a Multi-Head Attention mechanism and a feed-forward network, with skip connections (residual adds) and layer normalization applied. The decoder’s self-attention is *masked* (causal) so that each position can only attend to earlier positions, enabling autoregressive text generation. (In encoder-decoder models for tasks like translation, the decoder also includes cross-attention to encoder outputs; however, most LLMs from big players are **decoder-only** Transformers that rely solely on self-attention for generation.)*

### Self-Attention Mechanism

At the heart of the Transformer (and any LLM using it) is the **self-attention** mechanism. Self-attention allows the model to weigh the relevance of different words in the input to each other, effectively learning which other words (earlier in the sequence) are important when producing a representation for a given word. The mechanism works as follows: each input token’s embedding is projected into three vectors – a **Query**, a **Key**, and a **Value** – via learned linear transformations. For a given token (position) we compute how much attention it should pay to every other token by taking the dot product of its Query vector with each other token’s Key vectors. This yields a set of *attention scores* (one score per pair of tokens); after scaling and applying a softmax, these scores become weights that sum to 1. Each token then computes a weighted sum of all other tokens’ Value vectors, using those attention weights. The output of the self-attention for that token is this weighted combination of Values – effectively a context-sensitive representation that emphasizes tokens deemed relevant. Because the attention weights are learned (via training data) for each context, the model can dynamically focus on relevant words regardless of their distance in the sequence. This is a key innovation: **attention** enables the model to capture long-range dependencies (e.g. a word at the beginning of a paragraph influencing one at the end) by a direct weighting, rather than having to pass information step by step through each intermediate position as in an RNN. Importantly, this attention operation is done in parallel for all token positions using matrix operations, which is highly efficient on modern hardware.

For **autoregressive** LLMs (ones that generate text left-to-right, like GPT), the self-attention is configured to be *causal*. That means each position can only attend to positions *at or before itself* in the sequence – the model cannot look at “future” tokens that it hasn’t generated yet. This is implemented by a **causal mask** (a lower-triangular matrix of 0/–∞ values) that forces the attention weights for any future token to be zero. Thanks to this mask, during training the model learns to predict the next token using only past context, and at inference time it can generate text one token at a time without cheating by peeking ahead. (By contrast, in a non-generative encoder like BERT or in an encoder-decoder, attention can be bidirectional or cross-attentive, but in LLMs for generation the causal mask is always applied to self-attention.)

### Multi-Head Attention

Instead of performing a single attention operation, Transformers use **Multi-Head Attention (MHA)** – they compute multiple attention “heads” in parallel. Each head is an independent self-attention sublayer with its own Query, Key, Value projections. For example, if there are 12 heads, the model projects the input into 12 different sets of Q,K,V and computes 12 separate attention weight patterns. The outputs of all heads are then concatenated and linearly combined to form the final output of the attention layer. The intuition is that different heads can learn to focus on different types of relationships or patterns in the data (e.g. one head might attend strongly to the subject of a sentence when processing a verb, while another head might attend to an object, etc.). In standard multi-head attention, each head has its own Q, K, V projection matrices (i.e. its “unique set” of Q,K,V parameters), so each head can compute a distinct attention mapping. This greatly enriches the model’s capacity to represent complex patterns. The effect is analogous to multiple convolutional filters in CNNs – each head can capture a different aspect of the sequence’s information. After computing all heads, the Transformer merges them, allowing subsequent layers to use information from various attention perspectives.

**Scale of Attention:** Modern LLMs often have a very large number of attention heads and layers. For instance, OpenAI’s GPT-3 (175B) model uses 96 transformer layers, each with 96 attention heads, and an internal embedding size around 12k dimensions. This means every transformer block in GPT-3 performs 96 separate self-attention computations over the sequence and combines them. This enormous multi-head capacity is part of what allows GPT-3 and similar models to capture the richness of language patterns. However, a downside is that the **computational cost** of attention grows with the square of the sequence length (O(*n*²) for sequence length *n*), since each token attends to all others. Memory usage is also quadratic in *n* for storing the attention weights. We will discuss later how researchers mitigate these costs for very long sequences.

### Positional Encoding

One challenge with the attention mechanism is that it is *permutation-invariant*: if we were to shuffle the order of the input tokens, the set of Q,K,V vectors would be the same, and the attention mechanism itself has no inherent sense of sequence order. But word order is obviously critical in language. Transformers address this by adding a **positional encoding** to the input embeddings to inject information about token positions. In the original Transformer, this was done via **sinusoidal positional encodings** – fixed sinusoid waves of different frequencies for each dimension, so that each position *i* in the sequence has a unique sinusoidal vector added to the token’s embedding. Other implementations use **learned positional embeddings**, which are simply additional learned vectors for each position index. By adding (or concatenating) position information, the model’s attention mechanism can distinguish between tokens at position 1 vs position 50, etc., and thus is sensitive to word order.

Modern LLMs have improved on positional encodings for better performance and ability to handle longer contexts. Many use **rotary position embeddings (RoPE)**, which rotate the Q and K vectors through a position-dependent angle rather than adding a vector. This technique (used in GPT-3 and LLaMA, for example) smoothly encodes positions and has some advantages in extrapolating to sequences longer than seen in training. Another approach is **relative positional encoding**, where instead of absolute positions, the attention mechanism learns biases based on the *distance* between tokens. Transformer-XL and later models use relative position embeddings so that the model can generalize to longer sequences by understanding “this token is 5 steps ahead of that token” regardless of absolute position. A very effective recent method is **ALiBi (Attention with Linear Biases)**, which forgoes explicit positional embeddings and instead biases the attention scores by a term proportional to the distance between tokens. ALiBi was found to enable better length extrapolation – models trained on shorter sequences can generalize to longer ones with less degradation. In summary, positional encoding is a crucial component that gives the model a sense of word order, and ongoing research (like ALiBi, Rotary, etc.) aims to extend the range and robustness of position representations in LLMs.

### Feed-Forward Networks and Residuals

After the attention sub-layer, each Transformer block includes a **position-wise feed-forward network** (FFN). This is simply a multi-layer perceptron applied independently to each token’s representation. In practice, it’s a 2-layer linear network: one linear projection to expand the dimensionality (often by a factor of 4), a non-linear activation, then a second linear projection back to the original dimensionality. For example, if the model’s embedding size is 1024, the FFN might expand to 4096 dimensions in the hidden layer, apply an activation (like ReLU or GELU), then project back to 1024. This provides additional transformation capacity at each layer, allowing the model to mix and refine the information that was integrated by the attention step. Importantly, the FFN operates on each token’s representation *separately* (it does not mix information between different token positions – that mixing is done by the attention sub-layer). Despite this per-token independence, the FFN is critical: it lets the model implement complex non-linear transformations and feature combinations for each token after attending to context.

Modern LLM architectures have found that using **gated activation units** in the feed-forward layer improves performance. Instead of a plain ReLU, variants like **GEGLU** or **SwiGLU** (Sigmoid-Weighted Linear Unit) are used, which effectively learn a gate to modulate the FFN output. In fact, Google’s PaLM and Meta’s LLaMA models both adopt SwiGLU activations in their feed-forward layers to boost model expressiveness. These activation variants (introduced in 2020 by Shazeer) have shown consistent gains over ReLU/GELU in large transformers, contributing to the strong performance of state-of-the-art LLMs.

Each Transformer sub-layer (attention or FFN) is wrapped with **residual skip-connections** and **layer normalization**. That is, the input of the sub-layer is added to its output (residual add), and then normalized (typically via LayerNorm) to stabilize training. Residual connections ensure that the gradient can flow easily through many layers (mitigating the vanishing gradient problem in deep networks) and allow each layer to incrementally refine the representations. Layer normalization helps keep activations at a scale that is easier to train, which is especially important given the depth (50+ layers) of modern LLMs. Notably, GPT-2 and later models use a **pre-normalization** setup (applying LayerNorm at the *input* of each sub-layer, rather than after the add) which tends to improve training stability for very deep models. Either way, the combination of residuals and normalization is key to successfully training extremely deep Transformers.

In summary, a Transformer layer in an LLM takes in a sequence of token representations, then: (1) updates each token by attending to other tokens (self-attention), and (2) further transforms each token in isolation (feed-forward network). Stacking many such layers builds up very rich representations. After the final layer, a linear output layer (language modeling head) projects the tokens’ representations to vocabulary logits, which are turned into probabilities via softmax – these give the predictive distribution for the next token.

## Autoregressive Text Generation Process

Once an LLM is trained, we use it to **generate text** by employing it in an autoregressive loop. The process typically works as follows:

1. **Tokenization and Input:** The input prompt or context (e.g. a user’s question) is first tokenized into the model’s tokens. These tokens are fed into the Transformer model in a single forward pass. Because of the causal attention mask, the model will produce a probability distribution over possible next tokens (for the position after the last input token).

2. **Next Token Prediction:** The model’s output probabilities are used to select the next token in the sequence. The simplest method is *greedy decoding* – pick the token with highest probability. Often, more sophisticated sampling methods are used (like multinomial sampling with temperature, top-k or top-p sampling) to produce more varied or appropriate outputs, especially in a creative context. Regardless of method, one token is chosen as the model’s next output.

3. **Iteration:** This newly generated token is then appended to the input sequence, and the updated sequence (old prompt + new token) is fed back into the model to predict the following token. This iterative process continues token by token, allowing the model to “roll out” a complete sentence or paragraph. Autoregressive LLMs thus operate like powerful next-word autocompleters, gradually building up a response.

A naïve implementation of this generation loop would be *extremely slow*, because each time we add a new token and feed the sequence, the Transformer would recompute all the intermediate values (Q,K,V, attention outputs, etc.) for the entire sequence from scratch. Fortunately, there is a crucial optimization: **key-value caching** for the attention layers. At each Transformer layer, the self-attention module produces Key and Value vectors for each token. When generating text step by step, these Key/Value vectors for past tokens can be *cached* and reused in subsequent steps. This means when the model generates the next token, it only needs to compute the new token’s Query at each layer and attend to the stored Keys and Values of the previous tokens, instead of recomputing keys/values for the entire past sequence. Using a KV cache avoids redundant computation of earlier tokens’ contributions. In practical terms, this yields an enormous speed-up for long sequences: without caching, generating *n* tokens would require O(*n*²) total operations (since each step recomputes length-*n* attentions), whereas with caching it is O(*n*·L) where *L* is the number of layers (each new token only does attention *to* past tokens, which is O(*n*) per layer). The memory trade-off is that we must store these past Keys and Values for each layer – which can be quite large for big models. For example, a GPT-3 model with 175B parameters and a context of 2048 tokens would require on the order of a few GB of memory just to store the KV cache for one generation . But this is a necessary cost for feasible runtime. (Techniques to reduce KV memory, like multi-query attention, will be discussed later.)

During generation, the model’s output at each step is a probability distribution over the vocabulary. The model doesn’t “plan” the whole sentence in advance; it makes a decision one token at a time, each conditioned on all prior tokens (which is why the context window limitation is important – it cannot condition on more tokens than fit in the window). Despite this myopic process, large Transformers exhibit an emergent ability to maintain coherence, follow instructions, and produce high-level logical outputs, thanks to the knowledge and patterns distilled in their weights.

It’s worth noting that at inference time, LLMs typically run in half-precision or even quantized integer forms to speed up computation and reduce memory. The generation algorithm, however, remains the same. Also, developers often incorporate **decoding strategies** (like blocking certain repetitions or biases) to improve output quality, but those are external techniques on top of the core model’s next-token predictions.

## Long Contexts and Efficient Attention Mechanisms

One of the frontiers of LLM development is **increasing the context window size** – the number of tokens the model can attend to at once. Early Transformers like the original GPT-2 had a context window of 1024 tokens. Today, models like GPT-4 and Anthropic’s Claude can handle contexts of 32k, 100k tokens or more, and Google has even demonstrated prototypes with up to **1 million** token windows. Handling such long sequences is challenging because of the aforementioned O(*n*²) scaling of vanilla attention. Researchers have approached this problem through various architectural and algorithmic innovations:

* **Modified Positional Encoding for Extrapolation:** Techniques like **ALiBi** and **xPos** modify how positions are encoded so that models can generalize to sequence lengths longer than those seen in training. ALiBi, for instance, introduces a distance-dependent bias in the attention scores instead of absolute position embeddings, and has shown *“superior extrapolation”* to longer sequences beyond the training limit. Likewise, rotary embeddings (RoPE) and its enhancements (such as an exponential decay on angles in xPos) help the model not to “forget” how to attend when sequences get longer than expected. By improving positional representations, these methods extend context length without changing the fundamental quadratic attention algorithm. Many current LLMs (especially those fine-tuned for longer contexts) leverage such positional tweaks; for example, GPT-4’s 32k context model reportedly uses a variant of position embeddings that allows it to scale up context length.

* **Recurrence and Segment Processing:** Another approach is to break a long input into segments and allow the model to carry information forward between segments. **Transformer-XL** (Dai et al., 2019) introduced a *segment-level recurrence* mechanism: the model maintains a memory of past segment representations, which are used as an extension of the context for the current segment. It also used relative positional encoding so that the model can seamlessly attend from the current segment to tokens in previous segments. This effectively lets a Transformer model have an unlimited context in theory, since it can process text in chunks but preserve a state that carries over long-term dependencies. Extending this idea, **Compressive Transformers** (2019) went further by compressing old memories (to save space) but still retaining some influence of very distant context. These recurrent memory approaches make the model kind of a hybrid between a Transformer and an RNN, gaining some benefits of each. In practice, segment-based methods can greatly extend context (Transformer-XL saw significant perplexity gains on long sequences by using memory). The downside is added complexity – the model must be trained to use the memory effectively, and generation becomes a bit more complex (needing to manage the rolling memory). Nonetheless, recurrence is a promising direction and may be key as we push context lengths to documents or even book-level lengths.

* **Sliding Windows and Chunked Processing:** In real-world usage, if a conversation or document exceeds a model’s context window, there are simpler heuristic approaches like moving windows or summarization. For example, a chat system might summarize older parts of the conversation once it falls out of the window, and prepend that summary as context for later messages. Or it may use a *sliding window* to process a long text in chunks with overlap and aggregate the outputs. These are not changes to the model architecture but rather algorithmic strategies at the application level. They highlight that making use of ultra-long texts may sometimes require hierarchical processing. (In fact, Anthropic’s 100k-token Claude uses a clever retrieval and summarization strategy internally to handle such long inputs, in addition to any architectural optimizations.) A straightforward observation is that simply increasing the context size has diminishing returns and potential downsides – **making the context window too large can dilute the model’s focus** or make training unstable. Thus, long-context handling often involves *both* architectural changes to the model and external strategies like retrieval or summarization.

* **Sparse and Local Attention Patterns:** A powerful idea to reduce the O(*n*²) cost is to not let every token attend to every other token. Many **efficient Transformer** variants use sparse attention patterns. For example, **Longformer** uses a combination of local sliding window attention (each token attends only to neighbors within a fixed window) and a few *global* tokens that can attend broadly. This makes attention complexity linear in sequence length for long texts, yet the global tokens can propagate info across far parts. **BigBird** (Google, 2020) similarly uses a mix of local and random sparse attention and proved that such patterns are Turing-complete and asymptotically allow attending to all tokens eventually. **Block-sparse** attention is another variant, where the sequence is divided into blocks and attention is restricted to a pattern of blocks (somewhat like dividing the attention matrix into a sparse matrix). OpenAI’s earlier research on Sparse Transformers (2019) explored fixed patterns like strided attention (each token attends to every *k*th token, etc.). These approaches drastically cut computation for long inputs. The trade-off is that the model might miss some long-distance connections because not every pair can directly interact in one layer. However, with multiple layers (and possibly rotating which positions attend where at different layers), information can still flow. Notably, sparse attention models can handle, say, 8k or 16k tokens with the compute equivalent of a dense attention handling 1k or 2k tokens, which is a big win. Several long-document models (for tasks like long text summarization) use these ideas, and techniques like Longformer have been popular for extending BERT-like models to long documents.

* **Low-Rank and Approximate Attention:** Another category is to approximate the full attention calculation. **Linformer** (2020) showed that one can project the length dimension to a smaller size (e.g. project the Keys and Values via a low-rank projection) without too much loss, making attention effectively O(*n*) instead of O(*n*²). **Performer** (2020) uses a kernel approximation technique to make softmax-attention linear time – it maps the softmax(Q·Kᵀ) computation into a feature space where attention can be computed with linear complexity (this is sometimes called *linear attention*). **Reformer** (2020, from Google) uses locality-sensitive hashing to group similar queries and only attend within those groups, reducing complexity to O(*n* log *n*). There are also approaches like **Sparse Transformers** with learnable sparsity, and mixtures like **Routing Transformers**. Each of these has pros/cons and often some drop in accuracy compared to full attention, but they enable handling very long sequences that would otherwise be infeasible. A recent survey identified dozens of such efficient attention proposals. While not all are used in production LLMs, these ideas are influencing the design of next-generation models that aim to be both fast and long-context.

* **Memory-Efficient Attention Implementations:** Even if we keep the full attention mechanism, we can *optimize* how it’s implemented. A prime example is **FlashAttention**, an algorithm that reorders and fuses GPU operations to compute exact attention using much less memory and with better cache utilization. FlashAttention tiles the attention computation so that chunks of the attention matrix are computed and immediately used, rather than materializing the entire *n×n* matrix in memory. It also fuses multiple steps (like computing softmax and multiplying by values) into one GPU kernel. The result is that FlashAttention runs faster *and* can handle longer sequences on a given GPU without running out of memory. In one experiment, FlashAttention allowed training a model with a sequence 4× longer than the standard implementation, with equal or greater speed. In practice, nearly all new LLMs use some variant of FlashAttention or similar optimizations under the hood. This doesn’t change the model’s output at all (it’s mathematically exact attention), but it substantially improves efficiency, which indirectly facilitates using larger context windows on available hardware. Additionally, distributed attention schemes (splitting the sequence across multiple GPUs for the attention calculation) are being explored to push context lengths even further by utilizing more hardware in parallel.

* **Reducing Memory with Multi-Query Attention:** As mentioned earlier, storing the Key/Value cache for many attention heads is memory-intensive. **Multi-Query Attention (MQA)** is a modification to standard multi-head attention that greatly reduces this burden. In MQA, each head still has its own Query projection, *but all heads share a single Key projection and a single Value projection*. In other words, instead of 96 different Key/Value sets for 96 heads, the model has one Key and one Value for all heads (or sometimes a small number of shared groups – see below). At runtime, this means the Keys and Values for a sequence have shape independent of number of heads, drastically cutting the size of stored KV tensors. For example, if a model has *H* heads, Multi-Query Attention reduces KV memory by about a factor of *H*. Empirically, researchers found that MQA has minimal impact on model quality while saving huge memory. Google’s PaLM 540B model adopted MQA for faster inference. In fact, in the largest GPT-3 model (with 96 heads), using MQA would shrink the KV cache from an estimated \~4.5 GB to only \~48 MB for a full context – an astounding reduction. A generalization of this idea is **Grouped-Query Attention (GQA)**, where you have *G* groups of heads. Within each group, heads share one Key/Value set. Setting G = 1 is equivalent to MQA (all heads share), while G = H gives standard MHA (each head its own). Intermediate values allow a trade-off between memory and expressiveness. Notably, Meta’s **LLaMA 2** and the **Falcon** models use GQA in their large variants (e.g. LLaMA-70B uses 8 groups for 32 heads, meaning 4 heads share each KV set). They found this provides significant memory savings with only a slight quality improvement over pure MQA. By reducing memory bottlenecks, MQA/GQA enable using longer contexts or larger batch sizes in deployment, which is one reason they’ve been quickly adopted in state-of-the-art LLM implementations.

In combination, the strategies above allow LLMs to handle longer inputs than previously possible. For instance, Anthropic’s 100k-token Claude likely uses a mixture of an efficient attention implementation, careful positional embeddings, and clever inference strategies (like selectively reading parts of the context) to achieve that milestone. Google’s Gemini (2024) with million-token context may rely on distributing the attention computation and using recurrence or retrieval to manage such an enormous window. It’s worth noting that simply giving a model a huge context window doesn’t guarantee it will *use* it effectively – research has shown that beyond a certain length, models struggle with truly long-term dependencies and may need explicit training on long sequences or hierarchically structured inputs. Therefore, extending context goes hand in hand with training methodology and sometimes architectural tweaks like those discussed.

## Modern LLM Architectural Trends and Innovations

Putting it all together, how are today’s top LLMs implemented under the hood? Almost all of them use the Transformer decoder architecture with the basic ingredients we’ve outlined: multi-head causal self-attention and feed-forward layers, learned positional encodings (often rotary or similar), residual connections, and deep stacking. What differentiates models from each other are details like size, training data, and a few architectural choices:

* **Scale and Depth:** Larger models have more layers and higher dimensionality. For example, GPT-3 175B has 96 layers of size 12288 (≈12k) with 96 heads, while its predecessor GPT-2 (1.5B) had 48 layers of size 1600 with 20 heads. Google’s PaLM 540B went up to 118 layers of size 18432 with 48 heads (plus using parallel layers as mentioned below). These models are all *dense* Transformers (every layer’s weights are used for every token). The general finding (scaling law) is that increasing depth, width, and data tends to improve performance, albeit with diminishing returns and huge computational cost.

* **Activation Functions:** As noted, many recent LLMs replaced GELU activations in feed-forward layers with **SwiGLU** for better performance. PaLM and LLaMA used SwiGLU and reported gains. These activation tweaks are now standard in new models.

* **Layer Normalization:** Most models use Pre-LN (normalize before attention and FF sublayers) as it aids training for deep networks. Some models also remove bias terms in LayerNorm and linear layers for simplicity; PaLM for example had no biases in its linear and norm layers, which Google found beneficial for stability.

* **Parallel or Fused Layers:** Google’s PaLM introduced an architecture variation where the attention and FFN computations are partially fused in parallel instead of strictly sequential within a block. This was done for efficiency: PaLM’s designers found you can intermix the computations and even fuse certain linear transformations from attention and FF into one, gaining speed with little loss. This kind of low-level architectural optimization doesn’t fundamentally change the model’s capability, but it shows the kind of refinements used in cutting-edge implementations to squeeze more performance.

* **Multi-Query/Grouped Attention:** As discussed, PaLM was an early large model to use **Multi-Query Attention**, reducing its memory and latency during inference. Meta’s LLaMA-2 and others followed with **Grouped-Query Attention** to balance quality and memory. These modifications are invisible to end-users (the model still outputs the same format of results), but they are important engineering choices that allow these models to be practical at scale.

* **Mixture of Experts (MoE):** A different line of architectural innovation is the Mixture-of-Experts technique. An MoE layer consists of multiple parallel FFN “expert” networks and a trainable gating network that chooses a few experts for each token. This way, the model’s parameter count can be huge (since there might be, say, 64 experts each as big as a normal FFN) but any given token only activates a few of them, so the computation per token remains manageable. Google’s Switch Transformer (2021) and GLaM (2022) used MoE to scale to trillions of parameters while actually *reducing* inference FLOPs compared to a dense model of equivalent size. For instance, GLaM had 1.2T weights but only a fraction were used per token. MoE models have shown very strong performance-per-compute, but they introduce complexity (load balancing the experts, increased memory usage, etc.). As of 2023-2024, most deployed chatbots (ChatGPT, Bard, etc.) are still dense models, not MoE – however, research is ongoing and it’s possible that behind the scenes some “GPT-4” style models could be ensembles or MoEs. OpenAI hasn’t disclosed GPT-4’s architecture details, but some speculate it might use expert mixture or other tricks to get its capability. In any case, MoE represents a notable architecture option: **sparsely activated networks** that dramatically expand capacity without a proportional increase in computation.

* **Beyond Transformers:** While the Transformer dominates, there are explorations of other architectures for language models. Some recent implementations have tried **RNN-based LLMs** or **state-space models**. For example, the RWKV model is an RNN that matches some Transformer behavior, and research like “Mamba” (2023) builds on state-space models (a class of model that can handle long sequences with linear recurrent updates). These approaches aim to get Transformer-level performance with potentially better long-range handling or efficiency. So far, none have clearly surpassed Transformers for large-scale language modeling, but they are intriguing because they might handle long contexts with different trade-offs (state-space models can handle very long sequences in theory without quadratic cost). As of 2025, virtually all top-performing LLMs are still Transformer-based, but the field is open to new architectures if they prove themselves. The mention of Mamba in literature and the continued interest in recurrence indicates the community is aware that Transformers are not the end of the story.

To conclude, Large Language Models work through a combination of **massive scale neural networks** and **sophisticated algorithms** that allow them to learn and use language. They implement the forward pass via the Transformer architecture – encoding text into high-dimensional vectors, mixing information with attention, and transforming it with feed-forward networks – all learned from data via backpropagation. When in use, they generate text one piece at a time, with mechanisms like masked self-attention ensuring coherence and caching ensuring efficiency. The advanced details such as multi-head attention, various positional encodings, long-context strategies (memory, sparse attention), and optimization tricks (FlashAttention, etc.) are all about *how to best leverage computation to capture the essence of language*. The current state-of-the-art LLMs by OpenAI, Google, Meta, and others largely share this DNA of a Transformer core with incremental refinements. As hardware and research progress, we can expect further extensions to context length, more efficient attention mechanisms, and perhaps new architectures – but understanding the components we’ve discussed (attention, feed-forwards, training via next-token prediction) provides a solid foundation for how LLMs **implement** their remarkable language abilities.

**Sources:** The descriptions above are based on the transformer architecture as detailed by Vaswani et al. (2017) and various improvements from recent research. The specific figures and examples (GPT-3’s layer counts, PaLM’s architectural choices, long-context benchmarks, etc.) come from publications and technical blogs in the field. The explanation of attention and caching draws on standard texts and analyses. These sources collectively illustrate both the fundamental algorithms (like backpropagation and multi-head attention) and the cutting-edge techniques enabling today’s large language models to function.
